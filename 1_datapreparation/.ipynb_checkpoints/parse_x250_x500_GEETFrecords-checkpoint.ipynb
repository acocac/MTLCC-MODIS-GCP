{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import glob\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODparser_write():\n",
    "    \"\"\" defined the  .tfrecord format \"\"\"\n",
    "    def __init__(self):\n",
    "\n",
    "        self.feature_format= {\n",
    "            'x250/data': tf.FixedLenFeature([], tf.string),\n",
    "            'x250/shape': tf.FixedLenFeature([4], tf.int64),\n",
    "            'x500/data': tf.FixedLenFeature([], tf.string),\n",
    "            'x500/shape': tf.FixedLenFeature([4], tf.int64),\n",
    "            'dates/doy': tf.FixedLenFeature([], tf.string),\n",
    "            'dates/year': tf.FixedLenFeature([], tf.string),\n",
    "            'dates/shape': tf.FixedLenFeature([1], tf.int64),\n",
    "            'labels/data': tf.FixedLenFeature([], tf.string),\n",
    "            'labels/shape': tf.FixedLenFeature([3], tf.int64)\n",
    "        }\n",
    "\n",
    "        return None\n",
    "\n",
    "    def write(self, filename, x250ds, x500ds, doy, year, labelsds):\n",
    "        # https://stackoverflow.com/questions/39524323/tf-sequenceexample-with-multidimensional-arrays\n",
    "\n",
    "        options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)\n",
    "        writer = tf.python_io.TFRecordWriter(filename, options=options)\n",
    "          \n",
    "        for x in range(x250ds.shape[0]):\n",
    "\n",
    "          x250=x250ds[x].astype(np.int64)\n",
    "          x500=x500ds[x].astype(np.int64)\n",
    "          doy=doy.astype(np.int64)\n",
    "          year=year.astype(np.int64)\n",
    "          labels=labelsds[x].astype(np.int64)\n",
    "          \n",
    "          # Create a write feature\n",
    "          feature={\n",
    "              'x250/data' : tf.train.Feature(bytes_list=tf.train.BytesList(value=[x250.tobytes()])),\n",
    "              'x250/shape': tf.train.Feature(int64_list=tf.train.Int64List(value=x250.shape)),\n",
    "              'x500/data' : tf.train.Feature(bytes_list=tf.train.BytesList(value=[x500.tobytes()])),\n",
    "              'x500/shape': tf.train.Feature(int64_list=tf.train.Int64List(value=x500.shape)),\n",
    "              'labels/data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[labels.tobytes()])),\n",
    "              'labels/shape': tf.train.Feature(int64_list=tf.train.Int64List(value=labels.shape)),\n",
    "              'dates/doy': tf.train.Feature(bytes_list=tf.train.BytesList(value=[doy.tobytes()])),\n",
    "              'dates/year': tf.train.Feature(bytes_list=tf.train.BytesList(value=[year.tobytes()])),\n",
    "              'dates/shape': tf.train.Feature(int64_list=tf.train.Int64List(value=doy.shape))\n",
    "          }\n",
    "\n",
    "          example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "          writer.write(example.SerializeToString())\n",
    "\n",
    "        writer.close()\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    def parse_example(self,serialized_example):\n",
    "        \"\"\"\n",
    "        example proto can be obtained via\n",
    "        filename_queue = tf.train.string_input_producer(filenames, num_epochs=None)\n",
    "        or by passing this function in dataset.map(.)\n",
    "        \"\"\"\n",
    "\n",
    "        feature = tf.parse_single_example(serialized_example, self.feature_format)\n",
    "        \n",
    "        # decode and reshape\n",
    "        x500 = tf.reshape(tf.decode_raw(feature['x250/data'], tf.int64),tf.cast(feature['x250/shape'], tf.int32))\n",
    "        x500 = tf.reshape(tf.decode_raw(feature['x500/data'], tf.int64),tf.cast(feature['x500/shape'], tf.int32))\n",
    "\n",
    "        doy = tf.reshape(tf.decode_raw(feature['dates/doy'], tf.int64), tf.cast(feature['dates/shape'], tf.int32))\n",
    "        year = tf.reshape(tf.decode_raw(feature['dates/year'], tf.int64), tf.cast(feature['dates/shape'], tf.int32))\n",
    "\n",
    "        labels = tf.reshape(tf.decode_raw(feature['labels/data'], tf.int64), tf.cast(feature['labels/shape'], tf.int32))\n",
    "\n",
    "        return x250, x500, doy, year, labels\n",
    "      \n",
    "    def read(self,filenames):\n",
    "        \"\"\" depricated! \"\"\"\n",
    "\n",
    "        if isinstance(filenames,list):\n",
    "            filename_queue = tf.train.string_input_producer(filenames, num_epochs=None)\n",
    "        elif isinstance(filenames,tf.FIFOQueue):\n",
    "            filename_queue = filenames\n",
    "        else:\n",
    "            print (\"please insert either list or tf.FIFOQueue\")\n",
    "\n",
    "        reader = tf.TFRecordReader()\n",
    "        f, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "        feature = tf.parse_single_example(serialized_example, features=self.feature_format)\n",
    "\n",
    "        # decode and reshape\n",
    "        x250 = tf.reshape(tf.decode_raw(feature['x250/data'], tf.int64),tf.cast(feature['x250/shape'], tf.int32))\n",
    "        x500 = tf.reshape(tf.decode_raw(feature['x500/data'], tf.int64),tf.cast(feature['x500/shape'], tf.int32))\n",
    "        \n",
    "        doy = tf.reshape(tf.decode_raw(feature['dates/doy'], tf.int64), tf.cast(feature['dates/shape'], tf.int32))\n",
    "        year = tf.reshape(tf.decode_raw(feature['dates/year'], tf.int64), tf.cast(feature['dates/shape'], tf.int32))\n",
    "\n",
    "        labels = tf.reshape(tf.decode_raw(feature['labels/data'], tf.int64), tf.cast(feature['labels/shape'], tf.int32))\n",
    "\n",
    "        return x250, x500, doy, year, labels\n",
    "\n",
    "    def read_and_return(self,filename):\n",
    "        \"\"\" depricated! \"\"\"\n",
    "\n",
    "        # get feature operation containing\n",
    "        feature_op = self.read([filename])\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            tf.global_variables_initializer()\n",
    "\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "            return sess.run(feature_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_fn(ds_250m, ds_500m, bands_250m, bands_500m, tfiles, psize, timesteps, version, batchsize, project):\n",
    "\n",
    "  parser = MODparser_write()\n",
    "\n",
    "  ds_250m = tf.data.TFRecordDataset(ds_250m, compression_type='GZIP')\n",
    "  ds_500m = tf.data.TFRecordDataset(ds_500m, compression_type='GZIP')\n",
    "\n",
    "  bands250m = bands_250m + ['DOY', 'year', version]\n",
    "\n",
    "  # Dictionary with names as keys, features as values.\n",
    "  featureNames = list(bands250m)\n",
    "\n",
    "  columns = [tf.FixedLenFeature([timesteps, psize, psize], tf.int64) for k in featureNames]\n",
    "\n",
    "  featuresDict250m = dict(zip(featureNames, columns))\n",
    "\n",
    "  # Dictionary with names as keys, features as values.\n",
    "  featureNames = list(bands_500m)\n",
    "\n",
    "  columns = [tf.FixedLenFeature([timesteps, psize/2, psize/2], tf.int64) for k in featureNames]\n",
    "\n",
    "  featuresDict500m = dict(zip(featureNames, columns))\n",
    "  \n",
    "  def parse_tfrecord250m(example_proto):\n",
    "      feat = tf.parse_single_example(example_proto, featuresDict250m)\n",
    "      \n",
    "      band_names = bands_250m\n",
    "\n",
    "      x250 = tf.stack([(feat[x]) for x in bands_250m], axis=0)\n",
    "      x250 = tf.transpose(x250, [1, 2, 3, 0])\n",
    "\n",
    "      #for predictions over all area wo mask\n",
    "      year = tf.to_float(tf.reduce_mean(feat.pop('year'), axis=[1, 2]))\n",
    "      DOY = tf.to_float(tf.reduce_mean(feat.pop('DOY'), axis=[1, 2]))\n",
    "\n",
    "      landcover = feat.pop(version)\n",
    "\n",
    "      return x250, DOY, year, landcover\n",
    "\n",
    "  def parse_tfrecord500m(example_proto):\n",
    "    \n",
    "      feat = tf.parse_single_example(example_proto, featuresDict500m)\n",
    "      \n",
    "      x500 = tf.stack([(feat[x]) for x in bands_500m], axis=0)\n",
    "      x500 = tf.transpose(x500, [1, 2, 3, 0])\n",
    "\n",
    "      return x500\n",
    "    \n",
    "  # Map the function over the dataset\n",
    "  parsedDataset250m = ds_250m.map(parse_tfrecord250m, num_parallel_calls=20)\n",
    "  parsedDataset500m = ds_500m.map(parse_tfrecord500m, num_parallel_calls=20)\n",
    "\n",
    "  parsedDataset250m = parsedDataset250m.batch(batchsize)\n",
    "  parsedDataset500m = parsedDataset500m.batch(batchsize)\n",
    "\n",
    "  iterator250m = parsedDataset250m.make_one_shot_iterator()\n",
    "  iterator500m = parsedDataset500m.make_one_shot_iterator()\n",
    "\n",
    "  nfiles = tfiles/batchsize\n",
    "  \n",
    "  if tfiles % batchsize != 0:\n",
    "    nfiles = nfiles + 1\n",
    "\n",
    "  filepaths=[\"{}/{}.gz\".format(project,i) for i in range(int(nfiles))]\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "\n",
    "      for t in range(int(nfiles)):\n",
    "        x250, DOY, year, labels = iterator250m.get_next()\n",
    "        x250, DOY, year, labels = sess.run([x250, DOY, year, labels])\n",
    "        \n",
    "        x500 = iterator500m.get_next()\n",
    "        x500 = sess.run([x500])\n",
    "        \n",
    "        print(x250.shape)\n",
    "        print(x500[0].shape)\n",
    "\n",
    "        parser.write(filepaths[t], x250, x500[0], DOY[0,:], year[0,:], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames_250m = sorted(glob.glob(os.path.join('data', 'tile_p240k0*.gz')),key=os.path.getmtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames_500m = sorted(glob.glob(os.path.join('data', 'tile_p120k0*.gz')),key=os.path.getmtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "28\n",
      "(4, 46, 240, 240, 2)\n",
      "(4, 46, 120, 120, 5)\n",
      "(4, 46, 240, 240, 2)\n",
      "(4, 46, 120, 120, 5)\n",
      "(4, 46, 240, 240, 2)\n",
      "(4, 46, 120, 120, 5)\n",
      "(4, 46, 240, 240, 2)\n",
      "(4, 46, 120, 120, 5)\n",
      "(4, 46, 240, 240, 2)\n",
      "(4, 46, 120, 120, 5)\n",
      "(4, 46, 240, 240, 2)\n",
      "(4, 46, 120, 120, 5)\n",
      "(4, 46, 240, 240, 2)\n",
      "(4, 46, 120, 120, 5)\n"
     ]
    }
   ],
   "source": [
    "options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)\n",
    "\n",
    "psize = 240\n",
    "timesteps = 46\n",
    "version = 'MCD12Q1v6'\n",
    "\n",
    "n_patches_first = sum(1 for _ in tf.python_io.tf_record_iterator(fileNames_250m[0], options=options))\n",
    "n_patches_last = sum(1 for _ in tf.python_io.tf_record_iterator(fileNames_250m[-1], options=options))\n",
    "\n",
    "batchsize = n_patches_first\n",
    "\n",
    "tfiles_250m = (n_patches_first * (len(fileNames_250m)-1)) + n_patches_last\n",
    "print(tfiles_250m)\n",
    "\n",
    "n_patches_first = sum(1 for _ in tf.python_io.tf_record_iterator(fileNames_500m[0], options=options))\n",
    "n_patches_last = sum(1 for _ in tf.python_io.tf_record_iterator(fileNames_500m[-1], options=options))\n",
    "\n",
    "tfiles_500m = (n_patches_first * (len(fileNames_500m)-1)) + n_patches_last\n",
    "print(tfiles_500m)\n",
    "\n",
    "assert(tfiles_250m == tfiles_500m)\n",
    "\n",
    "bands_250m = ['red', 'NIR']\n",
    "bands_500m = ['blue', 'green', 'SWIR1', 'SWIR2','SWIR3']\n",
    "\n",
    "# define directory to store the fake dataset\n",
    "project = 'out/raw/240/data10'\n",
    "if not os.path.exists(project):\n",
    "    os.makedirs(project)\n",
    "     \n",
    "parser_fn(fileNames_250m, fileNames_500m, bands_250m, bands_500m, tfiles_250m, psize, timesteps, version, batchsize, project)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
