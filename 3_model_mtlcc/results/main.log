From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:233: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '10']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 10.0, '_save_checkpoints_steps': 10.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000025E171748D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000025E171751E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 10.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:56: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:543: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:216: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:185: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:501: The name tf.train.LoggingTensorHook is deprecated. Please use tf.estimator.LoggingTensorHook instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:501: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.

Done calling model_fn.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:233: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '10']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 10.0, '_save_checkpoints_steps': 10.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000026EECD84860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000026EECD851E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 10.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:56: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:545: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:216: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:185: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:503: The name tf.train.LoggingTensorHook is deprecated. Please use tf.estimator.LoggingTensorHook instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:503: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.

Done calling model_fn.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:233: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '10']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 10.0, '_save_checkpoints_steps': 10.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000232B8F137B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000232B8F141E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 10.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:56: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:544: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:216: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:185: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:502: The name tf.train.LoggingTensorHook is deprecated. Please use tf.estimator.LoggingTensorHook instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:502: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.

Done calling model_fn.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:233: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '10']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 10.0, '_save_checkpoints_steps': 10.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000026F523347B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000026F52335158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 10.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:56: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:543: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:216: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:185: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:501: The name tf.train.LoggingTensorHook is deprecated. Please use tf.estimator.LoggingTensorHook instead.

Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
loss = 3.077117, step = 0
accuracy = 0.06342231
Saving checkpoints for 10 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-19T10:02:06Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-10
Running local_init_op.
Done running local_init_op.
Evaluation [1/10]
Evaluation [2/10]
Evaluation [3/10]
Evaluation [4/10]
Evaluation [5/10]
Evaluation [6/10]
Evaluation [7/10]
Evaluation [8/10]
Evaluation [9/10]
Evaluation [10/10]
Finished evaluation at 2019-10-19-10:02:10
Saving dict for global step 10: accuracy = 0.06608073, global_step = 10, loss = 3.006671
Saving 'checkpoint_path' summary for global step 10: ./AMZ/model\model.ckpt-10
Saving checkpoints for 20 into ./AMZ/model\model.ckpt.
Skip the current checkpoint eval due to throttle secs (600 secs).
Saving checkpoints for 30 into ./AMZ/model\model.ckpt.
Skip the current checkpoint eval due to throttle secs (600 secs).
Saving checkpoints for 40 into ./AMZ/model\model.ckpt.
Skip the current checkpoint eval due to throttle secs (600 secs).
Saving checkpoints for 50 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Skip the current checkpoint eval due to throttle secs (600 secs).
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-19T10:02:57Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-50
Running local_init_op.
Done running local_init_op.
Evaluation [1/10]
Evaluation [2/10]
Evaluation [3/10]
Evaluation [4/10]
Evaluation [5/10]
Evaluation [6/10]
Evaluation [7/10]
Evaluation [8/10]
Evaluation [9/10]
Evaluation [10/10]
Finished evaluation at 2019-10-19-10:03:00
Saving dict for global step 50: accuracy = 0.26372612, global_step = 50, loss = 2.5907059
Saving 'checkpoint_path' summary for global step 50: ./AMZ/model\model.ckpt-50
Loss for final step: 2.5325947.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:233: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '2']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 2.0, '_save_checkpoints_steps': 2.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000016C3F674898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000016C3F675158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 2.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:56: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:535: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:216: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:185: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:493: The name tf.train.LoggingTensorHook is deprecated. Please use tf.estimator.LoggingTensorHook instead.

Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
loss = 3.0942419, step = 0
accuracy = 0.05105252
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-19T10:08:48Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/2]
Evaluation [2/2]
Finished evaluation at 2019-10-19-10:08:50
Saving dict for global step 2: accuracy = 0.053710938, global_step = 2, loss = 3.0907602
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Skip the current checkpoint eval due to throttle secs (600 secs).
Saving checkpoints for 6 into ./AMZ/model\model.ckpt.
Skip the current checkpoint eval due to throttle secs (600 secs).
Saving checkpoints for 8 into ./AMZ/model\model.ckpt.
Skip the current checkpoint eval due to throttle secs (600 secs).
Saving checkpoints for 10 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Skip the current checkpoint eval due to throttle secs (600 secs).
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-19T10:09:13Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-10
Running local_init_op.
Done running local_init_op.
Evaluation [1/2]
Evaluation [2/2]
Finished evaluation at 2019-10-19-10:09:15
Saving dict for global step 10: accuracy = 0.055392794, global_step = 10, loss = 3.0751367
Saving 'checkpoint_path' summary for global step 10: ./AMZ/model\model.ckpt-10
Loss for final step: 3.0695572.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:233: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001A84BC428D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001A84BC43268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:56: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:535: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:216: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:185: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:493: The name tf.train.LoggingTensorHook is deprecated. Please use tf.estimator.LoggingTensorHook instead.

Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-19T10:17:32Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-19-10:17:33
Saving dict for global step 1: accuracy = 0.0656467, global_step = 1, loss = 3.081101
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0664902, step = 0
accuracy = 0.063964844
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Skip the current checkpoint eval due to throttle secs (600 secs).
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Skip the current checkpoint eval due to throttle secs (600 secs).
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Skip the current checkpoint eval due to throttle secs (600 secs).
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Skip the current checkpoint eval due to throttle secs (600 secs).
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-19T10:17:53Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-19-10:17:55
Saving dict for global step 5: accuracy = 0.06526693, global_step = 5, loss = 3.0777764
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
Loss for final step: 3.0530396.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:233: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1, '_save_checkpoints_steps': 1, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000241735E38D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000241735E41E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:233: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1, '_save_checkpoints_steps': 1, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002A813DE2828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002A813DE3268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:233: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1, '_save_checkpoints_steps': 1, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001D10C2828D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001D10C2831E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:56: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:535: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:216: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:185: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:493: The name tf.train.LoggingTensorHook is deprecated. Please use tf.estimator.LoggingTensorHook instead.

Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-19T10:24:50Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-19-10:24:51
Saving dict for global step 1: accuracy = 0.053493924, global_step = 1, loss = 3.0916214
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0606067, step = 0
accuracy = 0.06277127
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Skip the current checkpoint eval due to throttle secs (600 secs).
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Skip the current checkpoint eval due to throttle secs (600 secs).
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Skip the current checkpoint eval due to throttle secs (600 secs).
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Skip the current checkpoint eval due to throttle secs (600 secs).
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-19T10:25:11Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-19-10:25:12
Saving dict for global step 5: accuracy = 0.053656682, global_step = 5, loss = 3.0872886
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
Loss for final step: 3.0428715.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:233: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1, '_save_checkpoints_steps': 1, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000255D05F4828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000255D05F5158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:233: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1, '_save_checkpoints_steps': 1, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000256646328D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000256646331E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:56: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:535: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:216: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:185: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:493: The name tf.train.LoggingTensorHook is deprecated. Please use tf.estimator.LoggingTensorHook instead.

Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-19T10:26:27Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-19-10:26:28
Saving dict for global step 1: accuracy = 0.04671224, global_step = 1, loss = 3.1426191
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.03518, step = 0
accuracy = 0.065700956
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-19T10:26:37Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-19-10:26:38
Saving dict for global step 2: accuracy = 0.051974826, global_step = 2, loss = 3.1013238
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-19T10:26:44Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-19-10:26:46
Saving dict for global step 3: accuracy = 0.05202908, global_step = 3, loss = 3.1130846
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-19T10:26:51Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-19-10:26:52
Saving dict for global step 4: accuracy = 0.05360243, global_step = 4, loss = 3.085798
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-19T10:26:58Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-19-10:27:00
Saving dict for global step 5: accuracy = 0.057454426, global_step = 5, loss = 3.0666308
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
Loss for final step: 3.0057843.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:233: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '2']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 2.0, '_save_checkpoints_steps': 2.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000151FB4148D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000151FB415158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 2.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:56: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:535: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:216: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:185: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:493: The name tf.train.LoggingTensorHook is deprecated. Please use tf.estimator.LoggingTensorHook instead.

Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
loss = 3.0814013, step = 0
accuracy = 0.054253474
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-19T10:28:49Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/2]
Evaluation [2/2]
Finished evaluation at 2019-10-19-10:28:51
Saving dict for global step 2: accuracy = 0.049316406, global_step = 2, loss = 3.0829382
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-19T10:29:00Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/2]
Evaluation [2/2]
Finished evaluation at 2019-10-19-10:29:02
Saving dict for global step 4: accuracy = 0.055826824, global_step = 4, loss = 3.0582194
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
Saving checkpoints for 6 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-19T10:29:08Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-6
Running local_init_op.
Done running local_init_op.
Evaluation [1/2]
Evaluation [2/2]
Finished evaluation at 2019-10-19-10:29:10
Saving dict for global step 6: accuracy = 0.061577693, global_step = 6, loss = 3.0446033
Saving 'checkpoint_path' summary for global step 6: ./AMZ/model\model.ckpt-6
Saving checkpoints for 8 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-19T10:29:16Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-8
Running local_init_op.
Done running local_init_op.
Evaluation [1/2]
Evaluation [2/2]
Finished evaluation at 2019-10-19-10:29:18
Saving dict for global step 8: accuracy = 0.07004123, global_step = 8, loss = 3.0194201
Saving 'checkpoint_path' summary for global step 8: ./AMZ/model\model.ckpt-8
Saving checkpoints for 10 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-19T10:29:24Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-10
Running local_init_op.
Done running local_init_op.
Evaluation [1/2]
Evaluation [2/2]
Finished evaluation at 2019-10-19-10:29:26
Saving dict for global step 10: accuracy = 0.06765408, global_step = 10, loss = 3.0096364
Saving 'checkpoint_path' summary for global step 10: ./AMZ/model\model.ckpt-10
Loss for final step: 3.0085826.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:233: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '2']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 2.0, '_save_checkpoints_steps': 2.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000126FD6A48D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000126FD6A5158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 2.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:56: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:535: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:216: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:185: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model2.py:493: The name tf.train.LoggingTensorHook is deprecated. Please use tf.estimator.LoggingTensorHook instead.

Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
loss = 3.0513165, step = 0
accuracy = 0.05908203
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-19T10:46:41Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:148: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001530F443828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001530F4440D0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:56: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:58: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:535: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:216: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:185: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:493: The name tf.train.LoggingTensorHook is deprecated. Please use tf.estimator.LoggingTensorHook instead.

Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T05:58:02Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-05:58:03
Saving dict for global step 1: accuracy = 0.050564237, global_step = 1, loss = 3.1041298
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0977771, step = 0
accuracy = 0.053765193
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T05:58:12Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-05:58:13
Saving dict for global step 2: accuracy = 0.04844835, global_step = 2, loss = 3.1136203
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T05:58:19Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-05:58:20
Saving dict for global step 3: accuracy = 0.054307725, global_step = 3, loss = 3.0972257
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T05:58:26Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-05:58:27
Saving dict for global step 4: accuracy = 0.055284288, global_step = 4, loss = 3.068942
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T05:58:33Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-05:58:34
Saving dict for global step 5: accuracy = 0.055555556, global_step = 5, loss = 3.0696373
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
Loss for final step: 3.076852.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022F2B942908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000022F2B9430D0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\inputs.py:56: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:535: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:214: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:216: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:185: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:493: The name tf.train.LoggingTensorHook is deprecated. Please use tf.estimator.LoggingTensorHook instead.

Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T06:03:36Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-06:03:37
Saving dict for global step 1: accuracy = 0.06287977, global_step = 1, loss = 3.0809267
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0270488, step = 0
accuracy = 0.07210287
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T06:03:46Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-06:03:47
Saving dict for global step 2: accuracy = 0.06342231, global_step = 2, loss = 3.0166125
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T06:03:53Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-06:03:54
Saving dict for global step 3: accuracy = 0.076551646, global_step = 3, loss = 2.9847822
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T06:04:00Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-06:04:01
Saving dict for global step 4: accuracy = 0.06749132, global_step = 4, loss = 3.0417578
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T06:04:07Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-06:04:08
Saving dict for global step 5: accuracy = 0.071777344, global_step = 5, loss = 2.9893491
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
Loss for final step: 3.040725.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000025561C93908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000025561C941E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001C2698B38D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001C2698B4158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:535: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:214: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:216: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:493: The name tf.train.LoggingTensorHook is deprecated. Please use tf.estimator.LoggingTensorHook instead.

Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T06:09:44Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-06:09:46
Saving dict for global step 1: accuracy = 0.05419922, global_step = 1, loss = 3.0770972
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0816095, step = 0
accuracy = 0.055609807
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T06:09:54Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-06:09:55
Saving dict for global step 2: accuracy = 0.053222656, global_step = 2, loss = 3.07601
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T06:10:01Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-06:10:02
Saving dict for global step 3: accuracy = 0.055284288, global_step = 3, loss = 3.0759706
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T06:10:08Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-06:10:09
Saving dict for global step 4: accuracy = 0.055664062, global_step = 4, loss = 3.075805
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T06:10:15Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-06:10:16
Saving dict for global step 5: accuracy = 0.056043837, global_step = 5, loss = 3.0754652
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
Loss for final step: 3.070815.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000019230A728D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000019230A73158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:535: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T06:14:26Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-06:14:28
Saving dict for global step 1: accuracy = 0.0492079, global_step = 1, loss = 3.0918727
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0916266, step = 0
accuracy = 0.05202908
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T06:14:36Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-06:14:37
Saving dict for global step 2: accuracy = 0.05669488, global_step = 2, loss = 3.0800781
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T06:14:43Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-06:14:44
Saving dict for global step 3: accuracy = 0.054633245, global_step = 3, loss = 3.0802503
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T06:14:50Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-06:14:51
Saving dict for global step 4: accuracy = 0.05452474, global_step = 4, loss = 3.0797467
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T06:14:57Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-06:14:58
Saving dict for global step 5: accuracy = 0.052517362, global_step = 5, loss = 3.0793383
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
Loss for final step: 3.07038.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000027DFEC72908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000027DFEC73158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:540: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000249C9541908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000249C95421E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:539: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001FE6C8C2828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001FE6C8C31E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:534: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000019E716A2828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000019E716A3158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:534: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T13:52:14Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-13:52:16
Saving dict for global step 1: accuracy = 0.05669488, global_step = 1, loss = 3.0796175
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0732841, step = 0
accuracy = 0.05669488, global_step = 0, loss = 3.0732841
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T13:52:23Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-13:52:24
Saving dict for global step 2: accuracy = 0.056369357, global_step = 2, loss = 3.0775316
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.11338976, global_step = 1, loss = 3.071197 (7.022 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T13:52:30Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-13:52:32
Saving dict for global step 3: accuracy = 0.056857638, global_step = 3, loss = 3.0751219
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.17051867, global_step = 2, loss = 3.0690196 (7.134 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T13:52:37Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-13:52:38
Saving dict for global step 4: accuracy = 0.05718316, global_step = 4, loss = 3.0723605
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.22873265, global_step = 3, loss = 3.0667965 (6.869 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-20T13:52:44Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-20-13:52:45
Saving dict for global step 5: accuracy = 0.05669488, global_step = 5, loss = 3.0694144
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
accuracy = 0.28716365, global_step = 4, loss = 3.064161 (6.929 sec)
Loss for final step: 3.064161.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001FCFD2727B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001FCFD2730D0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:534: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T03:45:05Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-03:45:07
Saving dict for global step 1: accuracy = 0.065592445, global_step = 1, loss = 3.0682955
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0767024, step = 0
accuracy = 0.060763888, global_step = 0, loss = 3.0767024
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T03:45:14Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-03:45:15
Saving dict for global step 2: accuracy = 0.06618924, global_step = 2, loss = 3.067264
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.12261285, global_step = 1, loss = 3.0747137 (7.091 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T03:45:21Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-03:45:23
Saving dict for global step 3: accuracy = 0.06689453, global_step = 3, loss = 3.0663664
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.1851671, global_step = 2, loss = 3.071644 (7.186 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T03:45:28Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-03:45:30
Saving dict for global step 4: accuracy = 0.06857639, global_step = 4, loss = 3.065398
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.25054252, global_step = 3, loss = 3.0677416 (6.958 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T03:45:35Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-03:45:37
Saving dict for global step 5: accuracy = 0.069118924, global_step = 5, loss = 3.064988
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
Performing the final export in the end of training.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000205403B27F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000205403B31E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:534: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T03:59:36Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-03:59:37
Saving dict for global step 1: accuracy = 0.04720052, global_step = 1, loss = 3.1716533
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.078992, step = 0
accuracy = 0.065212674, global_step = 0, loss = 3.078992
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T03:59:45Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-03:59:46
Saving dict for global step 2: accuracy = 0.048502605, global_step = 2, loss = 3.169286
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.13275824, global_step = 1, loss = 3.07122 (7.057 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T03:59:52Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-03:59:53
Saving dict for global step 3: accuracy = 0.04969618, global_step = 3, loss = 3.1664991
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.2022027, global_step = 2, loss = 3.0667562 (7.089 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T03:59:59Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:00:00
Saving dict for global step 4: accuracy = 0.049641926, global_step = 4, loss = 3.1627629
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.2744683, global_step = 3, loss = 3.0610592 (6.910 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:00:06Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:00:07
Saving dict for global step 5: accuracy = 0.04871962, global_step = 5, loss = 3.157417
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
Performing the final export in the end of training.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002A0BD012780>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002A0BD0131E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:534: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000016D1AC22780>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000016D1AC23268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:534: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:02:14Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:02:16
Saving dict for global step 1: accuracy = 0.061197918, global_step = 1, loss = 3.071964
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0784657, step = 0
accuracy = 0.059136286, global_step = 0, loss = 3.0784657
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:02:23Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:02:24
Saving dict for global step 2: accuracy = 0.06315104, global_step = 2, loss = 3.0657642
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.120659724, global_step = 1, loss = 3.0715175 (7.074 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:02:30Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:02:32
Saving dict for global step 3: accuracy = 0.06602648, global_step = 3, loss = 3.0594032
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.1867947, global_step = 2, loss = 3.0653403 (7.154 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:02:37Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:02:39
Saving dict for global step 4: accuracy = 0.06705729, global_step = 4, loss = 3.054803
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.25254992, global_step = 3, loss = 3.0633154 (6.927 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:02:44Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:02:46
Saving dict for global step 5: accuracy = 0.06890191, global_step = 5, loss = 3.0497065
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
Performing the final export in the end of training.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:122: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000019EB72347B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000019EB7235158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002592D8B3748>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002592D8B41E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:534: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:12:31Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:12:33
Saving dict for global step 1: accuracy = 0.057454426, global_step = 1, loss = 3.072019
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0781958, step = 0
accuracy = 0.05343967, global_step = 0, loss = 3.0781958
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:12:40Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:12:41
Saving dict for global step 2: accuracy = 0.057942707, global_step = 2, loss = 3.071036
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.10823567, global_step = 1, loss = 3.0767488 (7.014 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:12:47Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:12:49
Saving dict for global step 3: accuracy = 0.058973525, global_step = 3, loss = 3.0693412
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.16373697, global_step = 2, loss = 3.0749903 (7.237 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:12:54Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:12:55
Saving dict for global step 4: accuracy = 0.057725694, global_step = 4, loss = 3.067801
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.2188585, global_step = 3, loss = 3.073104 (6.886 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:13:01Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:13:03
Saving dict for global step 5: accuracy = 0.057671443, global_step = 5, loss = 3.066706
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
Performing the final export in the end of training.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:123: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002030FF057B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002030FF061E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:534: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:15:06Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:15:07
Saving dict for global step 1: accuracy = 0.056477863, global_step = 1, loss = 3.0773382
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.078607, step = 0
accuracy = 0.054633245, global_step = 0, loss = 3.078607
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:15:15Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:15:16
Saving dict for global step 2: accuracy = 0.05875651, global_step = 2, loss = 3.0772038
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.10986328, global_step = 1, loss = 3.0748312 (6.987 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:15:22Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:15:23
Saving dict for global step 3: accuracy = 0.05669488, global_step = 3, loss = 3.076768
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.16704644, global_step = 2, loss = 3.069724 (7.212 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:15:29Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:15:30
Saving dict for global step 4: accuracy = 0.056043837, global_step = 4, loss = 3.0772798
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.22710504, global_step = 3, loss = 3.066639 (6.890 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:15:36Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:15:37
Saving dict for global step 5: accuracy = 0.05750868, global_step = 5, loss = 3.077249
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
Performing the final export in the end of training.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:123: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001CEF2BA47B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001CEF2BA5158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:534: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:21:28Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:21:30
Saving dict for global step 1: accuracy = 0.05517578, global_step = 1, loss = 3.0529413
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.1175575, step = 0
accuracy = 0.04953342, global_step = 0, loss = 3.1175575
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:21:37Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:21:39
Saving dict for global step 2: accuracy = 0.054904513, global_step = 2, loss = 3.0549402
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.100857206, global_step = 1, loss = 3.1111212 (7.030 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:21:44Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:21:46
Saving dict for global step 3: accuracy = 0.05501302, global_step = 3, loss = 3.053948
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.15039062, global_step = 2, loss = 3.1081991 (7.126 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:21:51Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:21:53
Saving dict for global step 4: accuracy = 0.056911893, global_step = 4, loss = 3.0541816
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.20258246, global_step = 3, loss = 3.0984623 (6.938 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:21:58Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:22:00
Saving dict for global step 5: accuracy = 0.05734592, global_step = 5, loss = 3.0602155
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
Performing the final export in the end of training.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:123: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F6B32427B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001F6B3244158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:534: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:25:03Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:25:04
Saving dict for global step 1: accuracy = 0.06000434, global_step = 1, loss = 3.0833788
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0816913, step = 0
accuracy = 0.0601671, global_step = 0, loss = 3.0816913
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:25:12Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:25:13
Saving dict for global step 2: accuracy = 0.059570312, global_step = 2, loss = 3.0825129
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.12109375, global_step = 1, loss = 3.0782595 (7.111 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:25:19Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:25:20
Saving dict for global step 3: accuracy = 0.06000434, global_step = 3, loss = 3.0813973
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.18326823, global_step = 2, loss = 3.0740423 (7.172 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:25:26Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:25:27
Saving dict for global step 4: accuracy = 0.060384113, global_step = 4, loss = 3.0808203
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.24495444, global_step = 3, loss = 3.0697622 (6.915 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:25:33Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:25:34
Saving dict for global step 5: accuracy = 0.05940755, global_step = 5, loss = 3.0817633
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
Performing the final export in the end of training.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:123: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000026EFE833898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000026EFE835158>) includes params argument, but params are not passed to Estimator.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:164: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000155BBA327B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000155BBA341E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:534: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:51:29Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:51:31
Saving dict for global step 1: accuracy = 0.071614586, global_step = 1, loss = 3.062012
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0754147, step = 0
accuracy = 0.0640191, global_step = 0, loss = 3.0754147
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:51:38Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:51:39
Saving dict for global step 2: accuracy = 0.071017794, global_step = 2, loss = 3.064023
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.12722439, global_step = 1, loss = 3.0795038 (7.026 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:51:45Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:51:46
Saving dict for global step 3: accuracy = 0.07269965, global_step = 3, loss = 3.06057
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.1929796, global_step = 2, loss = 3.0671523 (7.078 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:51:52Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:51:53
Saving dict for global step 4: accuracy = 0.07524957, global_step = 4, loss = 3.0487509
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.26220703, global_step = 3, loss = 3.05624 (6.816 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:51:59Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:52:00
Saving dict for global step 5: accuracy = 0.07774523, global_step = 5, loss = 3.0391622
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
Performing the final export in the end of training.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:124: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000231663B47B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000231663B5158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:534: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:59:07Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:59:08
Saving dict for global step 1: accuracy = 0.051974826, global_step = 1, loss = 3.0325541
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0850878, step = 0
accuracy = 0.05061849, global_step = 0, loss = 3.0850878
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:59:16Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:59:17
Saving dict for global step 2: accuracy = 0.05669488, global_step = 2, loss = 3.006013
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.104166664, global_step = 1, loss = 3.0538497 (7.071 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:59:23Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:59:24
Saving dict for global step 3: accuracy = 0.0655382, global_step = 3, loss = 2.9630516
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.16015625, global_step = 2, loss = 3.0064611 (7.042 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:59:30Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:59:31
Saving dict for global step 4: accuracy = 0.08930121, global_step = 4, loss = 2.9012444
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.22813585, global_step = 3, loss = 2.964728 (6.947 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T04:59:37Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-04:59:38
Saving dict for global step 5: accuracy = 0.10611979, global_step = 5, loss = 2.855698
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
Performing the final export in the end of training.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:124: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000232E2935860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000232E29360D0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:534: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:07:29Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:07:30
Saving dict for global step 1: accuracy = 0.067220055, global_step = 1, loss = 2.9457648
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0274463, step = 0
accuracy = 0.05392795, global_step = 0, loss = 3.0274463
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:07:38Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:07:39
Saving dict for global step 2: accuracy = 0.058539495, global_step = 2, loss = 2.9633205
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.109429255, global_step = 1, loss = 2.9999058 (7.046 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:07:45Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:07:46
Saving dict for global step 3: accuracy = 0.05886502, global_step = 3, loss = 2.949854
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.17019314, global_step = 2, loss = 2.968995 (7.169 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:07:52Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:07:53
Saving dict for global step 4: accuracy = 0.05886502, global_step = 4, loss = 2.9482145
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.23687065, global_step = 3, loss = 2.947794 (6.959 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:07:59Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:08:00
Saving dict for global step 5: accuracy = 0.06429037, global_step = 5, loss = 2.9280396
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
accuracy = 0.3111979, global_step = 4, loss = 2.9314945 (7.042 sec)
Loss for final step: 2.9314945.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:175: The name tf.train.CheckpointSaverHook is deprecated. Please use tf.estimator.CheckpointSaverHook instead.

Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:180: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:124: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F2F2B72828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001F2F2B74158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:534: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:11:43Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:11:45
Saving dict for global step 1: accuracy = 0.060818143, global_step = 1, loss = 3.0750024
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.065389, step = 0
accuracy = 0.061143663, global_step = 0, loss = 3.065389
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:11:52Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:11:54
Saving dict for global step 2: accuracy = 0.06125217, global_step = 2, loss = 3.068568
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.13232422, global_step = 1, loss = 3.033501 (6.986 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:11:59Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:12:01
Saving dict for global step 3: accuracy = 0.06217448, global_step = 3, loss = 3.0632088
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.20513238, global_step = 2, loss = 3.0057037 (7.133 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:12:06Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:12:08
Saving dict for global step 4: accuracy = 0.05777995, global_step = 4, loss = 3.0617743
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.27571616, global_step = 3, loss = 2.9950113 (6.887 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:12:13Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:12:15
Saving dict for global step 5: accuracy = 0.05859375, global_step = 5, loss = 3.0599425
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
accuracy = 0.34255642, global_step = 4, loss = 2.9863586 (7.048 sec)
Loss for final step: 2.9863586.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:187: The name tf.train.CheckpointSaverHook is deprecated. Please use tf.estimator.CheckpointSaverHook instead.

Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:192: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:133: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002643A7D1898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002643A7D3158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:534: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:16:51Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:16:53
Saving dict for global step 1: accuracy = 0.04893663, global_step = 1, loss = 3.1116843
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.1151602, step = 0
accuracy = 0.057128906, global_step = 0, loss = 3.1151602
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:17:00Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:17:02
Saving dict for global step 2: accuracy = 0.0500217, global_step = 2, loss = 3.1220746
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.107530385, global_step = 1, loss = 3.1079583 (7.021 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:17:07Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:17:09
Saving dict for global step 3: accuracy = 0.06906467, global_step = 3, loss = 3.02965
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.16845703, global_step = 2, loss = 3.085695 (7.189 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:17:14Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:17:16
Saving dict for global step 4: accuracy = 0.069118924, global_step = 4, loss = 3.0427878
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.24131945, global_step = 3, loss = 2.972531 (6.900 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:17:21Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:17:23
Saving dict for global step 5: accuracy = 0.06890191, global_step = 5, loss = 3.0524015
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
accuracy = 0.31266278, global_step = 4, loss = 3.0480242 (7.014 sec)
Loss for final step: 3.0480242.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:182: The name tf.train.CheckpointSaverHook is deprecated. Please use tf.estimator.CheckpointSaverHook instead.

Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:187: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:134: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000021703F838D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000021703F84158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:534: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:30:42Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:30:44
Saving dict for global step 1: accuracy = 0.057400174, global_step = 1, loss = 3.05528
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0732772, step = 0
accuracy = 0.054633245, global_step = 0, loss = 3.0732772
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:30:51Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:30:53
Saving dict for global step 2: accuracy = 0.058973525, global_step = 2, loss = 3.0544112
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.11040582, global_step = 1, loss = 3.0714848 (7.239 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:30:59Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:31:00
Saving dict for global step 3: accuracy = 0.058539495, global_step = 3, loss = 3.0535257
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.16748047, global_step = 2, loss = 3.0688868 (7.142 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:31:06Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:31:07
Saving dict for global step 4: accuracy = 0.059027776, global_step = 4, loss = 3.05275
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.22439235, global_step = 3, loss = 3.0656161 (6.911 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:31:13Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:31:14
Saving dict for global step 5: accuracy = 0.058213975, global_step = 5, loss = 3.0522783
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
accuracy = 0.28190103, global_step = 4, loss = 3.0620651 (7.048 sec)
Loss for final step: 3.0620651.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:182: The name tf.train.CheckpointSaverHook is deprecated. Please use tf.estimator.CheckpointSaverHook instead.

Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:187: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:125: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000021FE6802898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000021FE68040D0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:182: The name tf.train.CheckpointSaverHook is deprecated. Please use tf.estimator.CheckpointSaverHook instead.

Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:187: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:125: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000225243C3898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000225243C4268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:182: The name tf.train.CheckpointSaverHook is deprecated. Please use tf.estimator.CheckpointSaverHook instead.

Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:187: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:125: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F096E02898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001F096E031E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:534: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:36:02Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:36:03
Saving dict for global step 1: accuracy = 0.06939019, global_step = 1, loss = 3.020342
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0374527, step = 0
accuracy = 0.062282987, global_step = 0, loss = 3.0374527
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:36:11Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:36:12
Saving dict for global step 2: accuracy = 0.07042101, global_step = 2, loss = 3.0123532
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.12407769, global_step = 1, loss = 3.034503 (7.060 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:36:18Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:36:19
Saving dict for global step 3: accuracy = 0.07085504, global_step = 3, loss = 3.0010304
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.18652344, global_step = 2, loss = 3.0301318 (7.169 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:36:25Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:36:26
Saving dict for global step 4: accuracy = 0.07313368, global_step = 4, loss = 2.9873896
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.24940321, global_step = 3, loss = 3.0244102 (6.811 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:36:32Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:36:33
Saving dict for global step 5: accuracy = 0.07394748, global_step = 5, loss = 2.9740384
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
accuracy = 0.3119032, global_step = 4, loss = 3.0171928 (7.031 sec)
Loss for final step: 3.0171928.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:182: The name tf.train.CheckpointSaverHook is deprecated. Please use tf.estimator.CheckpointSaverHook instead.

Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:187: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:125: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022C7A9E4860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000022C7A9E51E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:534: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:38:22Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:38:24
Saving dict for global step 1: accuracy = 0.05392795, global_step = 1, loss = 3.091944
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0974073, step = 0
accuracy = 0.052571613, global_step = 0, loss = 3.0974073
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:38:31Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:38:33
Saving dict for global step 2: accuracy = 0.04926215, global_step = 2, loss = 3.124544
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.10774739, global_step = 1, loss = 3.0588102 (7.057 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:38:39Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:38:40
Saving dict for global step 3: accuracy = 0.06141493, global_step = 3, loss = 3.0885112
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.14675564, global_step = 2, loss = 3.1061153 (7.124 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:38:45Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:38:47
Saving dict for global step 4: accuracy = 0.0656467, global_step = 4, loss = 3.0829687
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.20269096, global_step = 3, loss = 3.0641038 (6.916 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:38:52Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:38:54
Saving dict for global step 5: accuracy = 0.06662326, global_step = 5, loss = 3.0840137
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
accuracy = 0.26850042, global_step = 4, loss = 3.0487103 (7.038 sec)
Loss for final step: 3.0487103.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:182: The name tf.train.CheckpointSaverHook is deprecated. Please use tf.estimator.CheckpointSaverHook instead.

Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:187: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:125: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000242E5362898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000242E5363268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:182: The name tf.train.CheckpointSaverHook is deprecated. Please use tf.estimator.CheckpointSaverHook instead.

Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:187: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:125: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000213D90127F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000213D90140D0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:182: The name tf.train.CheckpointSaverHook is deprecated. Please use tf.estimator.CheckpointSaverHook instead.

Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:187: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:125: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023A45203860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000023A452041E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:182: The name tf.train.CheckpointSaverHook is deprecated. Please use tf.estimator.CheckpointSaverHook instead.

Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:187: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:125: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001D5305F1898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001D5305F2268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000292CE2D4860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000292CE2D51E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:534: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000015F29EF48D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000015F29EF5158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000026695D74860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000026695D751E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:535: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:46:59Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:47:00
Saving dict for global step 1: accuracy = 0.057725694, global_step = 1, loss = 3.0604146
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0739276, step = 0
accuracy = 0.05593533, global_step = 0, loss = 3.0739276
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:47:08Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:47:09
Saving dict for global step 2: accuracy = 0.0578342, global_step = 2, loss = 3.061699
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.112087674, global_step = 1, loss = 3.060069 (7.111 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:47:15Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:47:16
Saving dict for global step 3: accuracy = 0.05875651, global_step = 3, loss = 3.0546684
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.17046441, global_step = 2, loss = 3.0571756 (7.077 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:47:22Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:47:23
Saving dict for global step 4: accuracy = 0.061523438, global_step = 4, loss = 3.0605261
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.23388672, global_step = 3, loss = 3.041584 (6.858 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:47:29Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:47:30
Saving dict for global step 5: accuracy = 0.069607206, global_step = 5, loss = 3.0336027
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
accuracy = 0.30088976, global_step = 4, loss = 3.0298777 (7.028 sec)
Loss for final step: 3.0298777.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:182: The name tf.train.CheckpointSaverHook is deprecated. Please use tf.estimator.CheckpointSaverHook instead.

Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:187: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:125: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002AB3A4F38D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002AB3A4F4158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001E4D97718D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001E4D97731E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000014DB6322898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000014DB63231E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001A3422E2898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001A3422E31E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000015C1FF72898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000015C1FF731E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:535: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:58:01Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:58:03
Saving dict for global step 1: accuracy = 0.05609809, global_step = 1, loss = 3.0886855
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0828962, step = 0
accuracy = 0.05908203, global_step = 0, loss = 3.0828962
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:58:10Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:58:11
Saving dict for global step 2: accuracy = 0.056043837, global_step = 2, loss = 3.0899277
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.11881511, global_step = 1, loss = 3.075237 (7.099 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:58:17Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:58:19
Saving dict for global step 3: accuracy = 0.056423612, global_step = 3, loss = 3.08986
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.18277995, global_step = 2, loss = 3.0668929 (7.139 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:58:24Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:58:26
Saving dict for global step 4: accuracy = 0.05626085, global_step = 4, loss = 3.0902224
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.24707031, global_step = 3, loss = 3.0659251 (6.997 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T05:58:31Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-05:58:33
Saving dict for global step 5: accuracy = 0.057996962, global_step = 5, loss = 3.0887005
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
accuracy = 0.31103516, global_step = 4, loss = 3.0635962 (7.044 sec)
Loss for final step: 3.0635962.
Create CheckpointSaverHook.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001C5ACDE37F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001C5ACDE4158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000025DF1C708D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000025DF1C72158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:535: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Starting evaluation at 2019-10-21T06:01:08Z
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001E7E3C21860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001E7E3C23158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:535: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Starting evaluation at 2019-10-21T06:04:36Z
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
Evaluation [1/1]
Finished evaluation at 2019-10-21-06:04:45
Saving dict for global step 5: accuracy = 0.057454426, global_step = 5, loss = 3.0834439
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:187: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:125: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F8B3410860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001F8B34121E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001BE2B8E0860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001BE2B8E2158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:187: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:125: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000233E5DC1860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000233E5DC3158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:188: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:126: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002133F480860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002133F4821E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:177: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:124: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000020E1E6D0860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000020E1E6D2158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:177: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:124: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002179FF807B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002179FF821E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:179: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000024A6F7607F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000024A6F762158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:190: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:142: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001EDCA20F7F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001EDCA2111E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:190: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:142: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000292156A0860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000292156A2158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:196: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:143: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000260C13B3828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000260C13B4158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:238: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:142: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000027689003828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000276890041E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:238: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:142: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F0EFCD3828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001F0EFCD4158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:238: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:142: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001C59B223828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001C59B224158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:238: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:142: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000019147FD4828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000019147FD5158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:240: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:144: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000251D8451860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000251D8453158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:199: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002A644782898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002A6447831E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:535: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T06:50:40Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-06:50:42
Saving dict for global step 1: accuracy = 0.07557508, global_step = 1, loss = 3.0213041
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0714903, step = 0
accuracy = 0.060763888, global_step = 0, loss = 3.0714903
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T06:50:49Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-06:50:50
Saving dict for global step 2: accuracy = 0.06966146, global_step = 2, loss = 3.0136392
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.12407769, global_step = 1, loss = 3.0509648 (7.050 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T06:50:56Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-06:50:58
Saving dict for global step 3: accuracy = 0.06705729, global_step = 3, loss = 2.9954093
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.18863933, global_step = 2, loss = 3.0230577 (7.289 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T06:51:03Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-06:51:05
Saving dict for global step 4: accuracy = 0.076822914, global_step = 4, loss = 2.9921067
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.25298393, global_step = 3, loss = 2.9875033 (6.885 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T06:51:10Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-06:51:12
Saving dict for global step 5: accuracy = 0.09564887, global_step = 5, loss = 3.0036838
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
accuracy = 0.3260091, global_step = 4, loss = 2.9513898 (7.098 sec)
Loss for final step: 2.9513898.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:199: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022FCC3917B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000022FCC393158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:535: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T06:53:56Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-06:53:58
Saving dict for global step 1: accuracy = 0.05707465, global_step = 1, loss = 3.0438433
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0641325, step = 0
accuracy = 0.056911893, global_step = 0, loss = 3.0641325
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T06:54:05Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-06:54:06
Saving dict for global step 2: accuracy = 0.061197918, global_step = 2, loss = 3.0302808
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.11583117, global_step = 1, loss = 3.0533803 (7.062 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T06:54:12Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-06:54:14
Saving dict for global step 3: accuracy = 0.06439887, global_step = 3, loss = 3.0134091
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.1789822, global_step = 2, loss = 3.0355096 (7.113 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T06:54:19Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-06:54:20
Saving dict for global step 4: accuracy = 0.06808811, global_step = 4, loss = 3.012628
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.25021702, global_step = 3, loss = 3.0180533 (6.887 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T06:54:26Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-06:54:28
Saving dict for global step 5: accuracy = 0.07118055, global_step = 5, loss = 3.0015497
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
accuracy = 0.32676867, global_step = 4, loss = 2.995776 (7.055 sec)
Loss for final step: 2.995776.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:199: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000196BA7B2828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000196BA7B4158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:199: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023774003860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000023774004158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000279A48D1860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000279A48D31E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000026FF1FC3860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000026FF1FC51E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:202: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002183ED83860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002183ED84158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000160AA564860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000160AA565158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:535: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:03:16Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:03:17
Saving dict for global step 1: accuracy = 0.05159505, global_step = 1, loss = 3.050748
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0945954, step = 0
accuracy = 0.046983507, global_step = 0, loss = 3.0945954
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:03:25Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:03:26
Saving dict for global step 2: accuracy = 0.05577257, global_step = 2, loss = 3.0404255
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.09586589, global_step = 1, loss = 3.081077 (7.011 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:03:32Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:03:33
Saving dict for global step 3: accuracy = 0.061197918, global_step = 3, loss = 3.0057323
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.1476237, global_step = 2, loss = 3.0653758 (7.125 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:03:39Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:03:40
Saving dict for global step 4: accuracy = 0.06939019, global_step = 4, loss = 2.9508374
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.20529515, global_step = 3, loss = 3.0341425 (6.860 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:03:46Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:03:47
Saving dict for global step 5: accuracy = 0.084472656, global_step = 5, loss = 2.9036899
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
accuracy = 0.27077907, global_step = 4, loss = 2.998896 (7.064 sec)
Loss for final step: 2.998896.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001FB79903860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001FB79904158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002177D951860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002177D953158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001968F3037B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001968F3041E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001CD0FBC37B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001CD0FBC4158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001D2275A27F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001D2275A31E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:538: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:11:14Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:11:15
Saving dict for global step 1: accuracy = 0.06640625, global_step = 1, loss = 3.0645359
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.060525, step = 0
accuracy = 0.067599826, global_step = 0, loss = 3.060525
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:11:23Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:11:24
Saving dict for global step 2: accuracy = 0.067599826, global_step = 2, loss = 3.0640318
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.13595921, global_step = 1, loss = 3.0581038 (7.039 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:11:30Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:11:31
Saving dict for global step 3: accuracy = 0.06765408, global_step = 3, loss = 3.0639157
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.20388456, global_step = 2, loss = 3.0554147 (7.081 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:11:37Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:11:38
Saving dict for global step 4: accuracy = 0.06819662, global_step = 4, loss = 3.0638747
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.27332902, global_step = 3, loss = 3.0530267 (6.922 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:11:44Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:11:45
Saving dict for global step 5: accuracy = 0.067979604, global_step = 5, loss = 3.0634642
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
accuracy = 0.34429255, global_step = 4, loss = 3.0505471 (7.088 sec)
Loss for final step: 3.0505471.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000280C6D547B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000280C6D55158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000196C72F37B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000196C72F41E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:536: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:18:23Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:18:24
Saving dict for global step 1: accuracy = 0.05967882, global_step = 1, loss = 3.0859523
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.178307, step = 0
accuracy = 0.05419922, global_step = 0, loss = 3.178307
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:18:32Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:18:33
Saving dict for global step 2: accuracy = 0.057454426, global_step = 2, loss = 3.110238
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.12326389, global_step = 1, loss = 3.1164212 (7.040 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:18:39Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:18:40
Saving dict for global step 3: accuracy = 0.051812068, global_step = 3, loss = 3.1265805
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.18999566, global_step = 2, loss = 3.0981529 (7.161 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:18:46Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:18:47
Saving dict for global step 4: accuracy = 0.050347224, global_step = 4, loss = 3.1249979
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.2602539, global_step = 3, loss = 3.0737448 (6.979 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:18:53Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:18:54
Saving dict for global step 5: accuracy = 0.0515408, global_step = 5, loss = 3.1228642
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
accuracy = 0.3372396, global_step = 4, loss = 3.0455194 (7.122 sec)
Loss for final step: 3.0455194.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001BB18174828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001BB181751E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000024406BA4828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000024406BA51E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:611: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:33:39Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:33:41
Saving dict for global step 1: accuracy = 0.059624568, global_step = 1, loss = 3.0682516
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.1135051, step = 0
accuracy = 0.04654948, global_step = 0, loss = 3.1135051
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:33:49Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:33:50
Saving dict for global step 2: accuracy = 0.05832248, global_step = 2, loss = 3.0644338
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.09863281, global_step = 1, loss = 3.1044803 (7.123 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:33:56Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:33:57
Saving dict for global step 3: accuracy = 0.05826823, global_step = 3, loss = 3.0499249
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.15370008, global_step = 2, loss = 3.0902834 (7.223 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:34:03Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:34:04
Saving dict for global step 4: accuracy = 0.059027776, global_step = 4, loss = 3.0486631
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.20328775, global_step = 3, loss = 3.0879788 (6.941 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:34:10Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:34:11
Saving dict for global step 5: accuracy = 0.0578342, global_step = 5, loss = 3.050446
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
accuracy = 0.25282118, global_step = 4, loss = 3.07692 (7.048 sec)
Loss for final step: 3.07692.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000219B0764860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000219B0765158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:611: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Starting evaluation at 2019-10-21T07:35:42Z
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001C7A5A947B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001C7A5A951E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001893CBF47B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001893CBF5268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:611: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Starting evaluation at 2019-10-21T07:37:21Z
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:37:29
Saving dict for global step 5: accuracy = 0.05441623, global_step = 5, loss = 3.0739574
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000018A68F33860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000018A68F34268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000016EC0664860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000016EC06651E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:611: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000282FB425860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000282FB4261E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:611: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Starting evaluation at 2019-10-21T07:40:02Z
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:40:11
Saving dict for global step 5: accuracy = 0.05045573, global_step = 5, loss = 3.0821435
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002074DCF57B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002074DCF61E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:611: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Starting evaluation at 2019-10-21T07:42:21Z
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000254F54047B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000254F54051E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000025CB3544860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000025CB3545268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000017C8E2B4860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000017C8E2B5268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002188D2E4860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002188D2E51E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001FB0CBB5860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001FB0CBB6268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F2F23037B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001F2F2304268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000215278A4860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000215278A5268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:29: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000252804E4860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000252804E51E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:200: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000026A426F37B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000026A426F4158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:613: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:48:29Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:48:31
Saving dict for global step 1: accuracy = 0.07080078, global_step = 1, loss = 3.0691912
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0366373, step = 0
accuracy = 0.07389323, global_step = 0, loss = 3.0366373
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:48:38Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:48:40
Saving dict for global step 2: accuracy = 0.072645396, global_step = 2, loss = 3.0636342
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.15847439, global_step = 1, loss = 3.00948 (7.057 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:48:45Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:48:47
Saving dict for global step 3: accuracy = 0.0734592, global_step = 3, loss = 3.0533705
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.24598524, global_step = 2, loss = 2.9992626 (7.151 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:48:52Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:48:54
Saving dict for global step 4: accuracy = 0.07877604, global_step = 4, loss = 3.041148
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.34120008, global_step = 3, loss = 2.987721 (6.918 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:48:59Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:49:01
Saving dict for global step 5: accuracy = 0.08772787, global_step = 5, loss = 3.0177052
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
accuracy = 0.4409722, global_step = 4, loss = 2.9739985 (7.072 sec)
Loss for final step: 2.9739985.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:198: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:126: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000019373833860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000193738341E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:198: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:126: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002EA09C52898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002EA09C53268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:198: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:126: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000242EE432860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000242EE433268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:198: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:126: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000021ED7E83860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000021ED7E841E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:198: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:126: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000203BDD23860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000203BDD24268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:619: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:59:43Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:59:45
Saving dict for global step 1: accuracy = 0.06982422, global_step = 1, loss = 3.0443754
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0560057, step = 0
accuracy = 0.06222873, global_step = 0, loss = 3.0560057
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:59:52Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-07:59:54
Saving dict for global step 2: accuracy = 0.074652776, global_step = 2, loss = 3.032899
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.12771267, global_step = 1, loss = 3.044237 (7.019 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T07:59:59Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-08:00:01
Saving dict for global step 3: accuracy = 0.07524957, global_step = 3, loss = 3.0233638
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.19851345, global_step = 2, loss = 3.0216093 (7.144 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T08:00:07Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-08:00:08
Saving dict for global step 4: accuracy = 0.07356771, global_step = 4, loss = 3.0157459
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.26757812, global_step = 3, loss = 3.0377626 (7.149 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T08:00:14Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-08:00:15
Saving dict for global step 5: accuracy = 0.071614586, global_step = 5, loss = 3.031783
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
accuracy = 0.34380424, global_step = 4, loss = 3.006109 (7.164 sec)
Loss for final step: 3.006109.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:198: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:126: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022C907E3828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000022C907E41E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:197: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:126: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001B8534327B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001B8534331E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:198: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:126: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000149BCEA27B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000149BCEA3268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:198: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:126: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000228DC1E27B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000228DC1E3268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000012A682B27B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000012A682B31E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:619: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-5
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file utilities to get mtimes.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
Saving checkpoints for 6 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
loss = 3.0526552, step = 5
accuracy = 0.061631944, global_step = 5, loss = 3.0526552
Loss for final step: 3.0526552.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:198: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:126: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002929DD337B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002929DD341E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:198: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:126: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000027EA8A817F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000027EA8A822F0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:198: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:126: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002802BFE27B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002802BFE31E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Create CheckpointSaverHook.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:198: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:126: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001A0917C47B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001A0917C51E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001ECA64547B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001ECA64551E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:314: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001AEF35D47B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001AEF35D51E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:314: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F1BDA457B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001F1BDA46268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:314: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F2CF8F5828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001F2CF8F61E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:314: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002AC229A4860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002AC229A51E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:314: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000026661295860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000266612961E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:312: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000020BD0AC4898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000020BD0AC5268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:312: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001B2CA6D47F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001B2CA6D61E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:312: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023CB6AC3898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000023CB6AC5268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000021099364898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000021099366268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000024805CA4898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000024805CA6268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:312: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000141E73547F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000141E73561E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:310: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001D867D03898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001D867D051E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:628: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:551: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.

Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-6
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000020FC1A94860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000020FC1A951E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:628: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:551: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.

Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-6
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000255F33D5860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000255F33D61E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:628: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:551: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.

Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-6
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F81C564860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001F81C566158>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:627: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:550: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.

Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-6
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000286AC325828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000286AC327268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:627: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:550: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.

Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-6
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000262668C5828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000262668C6268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:631: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-6
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001EDB5294898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001EDB5295268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:631: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-6
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000262CDCA38D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000262CDCA42F0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:631: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-6
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000012735FA47F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000012735FA5268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:631: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-6
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000213AA4347F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000213AA435268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:631: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-6
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000015D32A64828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000015D32A651E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:634: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '32', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001A839A84898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001A839A851E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:634: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./AMZ/model\model.ckpt.
Saving checkpoints for 1 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T16:04:55Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-1
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-16:04:56
Saving dict for global step 1: accuracy = 0.06304254, global_step = 1, loss = 3.0321329
Saving 'checkpoint_path' summary for global step 1: ./AMZ/model\model.ckpt-1
loss = 3.0574324, step = 0
accuracy = 0.050564237, global_step = 0, loss = 3.0574324
Saving checkpoints for 2 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T16:05:05Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-2
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-16:05:06
Saving dict for global step 2: accuracy = 0.06749132, global_step = 2, loss = 3.008874
Saving 'checkpoint_path' summary for global step 2: ./AMZ/model\model.ckpt-2
accuracy = 0.08935547, global_step = 1, loss = 3.0488133 (7.834 sec)
Saving checkpoints for 3 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T16:05:13Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-3
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-16:05:15
Saving dict for global step 3: accuracy = 0.06580946, global_step = 3, loss = 2.9930413
Saving 'checkpoint_path' summary for global step 3: ./AMZ/model\model.ckpt-3
accuracy = 0.12868923, global_step = 2, loss = 3.0162947 (8.054 sec)
Saving checkpoints for 4 into ./AMZ/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T16:05:21Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-4
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-16:05:22
Saving dict for global step 4: accuracy = 0.063802086, global_step = 4, loss = 2.980206
Saving 'checkpoint_path' summary for global step 4: ./AMZ/model\model.ckpt-4
accuracy = 0.1681315, global_step = 3, loss = 2.9841197 (7.668 sec)
Saving checkpoints for 5 into ./AMZ/model\model.ckpt.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-10-21T16:05:29Z
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
Evaluation [1/1]
Finished evaluation at 2019-10-21-16:05:30
Saving dict for global step 5: accuracy = 0.061143663, global_step = 5, loss = 2.9692152
Saving 'checkpoint_path' summary for global step 5: ./AMZ/model\model.ckpt-5
accuracy = 0.20784505, global_step = 4, loss = 2.9517496 (7.874 sec)
Loss for final step: 2.9517496.
Calling model_fn.
Done calling model_fn.
Graph was finalized.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000027ACD0437F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000027ACD044268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:634: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001FD6ED957B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001FD6ED961E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:634: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001B7242437F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001B7242442F0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:634: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--train_on', '2015', '--epochs', '5', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000290F61537F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000290F6154268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:634: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./AMZ/model\model.ckpt-5
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model2', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz//multiple', '--train_on', '2015', '--epochs', '1', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model2', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000236C1B237F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000236C1B24268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model2', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz//multiple', '--train_on', '2015', '--epochs', '1', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model2', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001174F5437F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001174F544268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model2', '--datadir', './sample/data/gz/24/multiple', '--train_on', '2015', '--epochs', '1', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model2', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000021409C847F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000021409C85268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model2', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/24/multiple', '--train_on', '2015', '--epochs', '1', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model2', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000026DCD1D4898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000026DCD1D52F0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:634: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model2', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--train_on', '2015', '--epochs', '1', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model2', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002BD4F4A47F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002BD4F4A5268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:634: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model2', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--train_on', '2015', '--epochs', '1', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model2', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001AF591E37F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001AF591E42F0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:634: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './AMZ/model2', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--train_on', '2015', '--epochs', '1', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './AMZ/model2', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002748AA047B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002748AA051E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:635: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './sample/model', '--datadir', './sample/data/gz/24/multiple', '--train_on', '2015', '--epochs', '1', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002AD6FED3828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002AD6FED4268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './sample/model', '--datadir', './sample/data/gz/24/multiple', '--train_on', '2015', '--epochs', '1', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002A47FDA4828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002A47FDA5268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './sample/model', '--datadir', 'E:/acocac/research/scripts/sample/data/gz/24/multiple', '--train_on', '2015', '--epochs', '1', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023721693828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000023721694268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './sample/model', '--datadir', './sample/data/24/multiple', '--train_on', '2015', '--epochs', '1', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000020E5D8C2860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000020E5D8C32F0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:635: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './sample/model', '--datadir', './sample/data/24/multiple', '--train_on', '2015', '--epochs', '1', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 190.0, '_save_checkpoints_steps': 190.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002A372E3FC88>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002A36EB8F378>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 190.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:635: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './sample/model', '--datadir', './sample/data/24/multiple', '--train_on', '2015', '--epochs', '1', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 190.0, '_save_checkpoints_steps': 190.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000026185FEF518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000026181D342F0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 190.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:635: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './sample/model', '--datadir', './sample/data/24/multiple', '--train_on', '2015', '--epochs', '1', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 190.0, '_save_checkpoints_steps': 190.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000028EBC156320>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000028EB80342F0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 190.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:634: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './sample/model', '--datadir', './sample/data/24/multiple', '--train_on', '2015', '--epochs', '1', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 190.0, '_save_checkpoints_steps': 190.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001849B1753C8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x00000184970452F0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 190.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:634: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './sample/model', '--datadir', './sample/data/24/multiple', '--train_on', '2015', '--epochs', '1', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 190.0, '_save_checkpoints_steps': 190.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001C8F59A6320>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001C8F18842F0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 190.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:636: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./sample/model\model.ckpt.
loss = 3.0184329, step = 0
accuracy = 0.088541664, global_step = 0, loss = 3.0184329
accuracy = 0.13020833, global_step = 1, loss = 3.086379 (1.182 sec)
accuracy = 0.19444445, global_step = 2, loss = 3.0873783 (0.248 sec)
accuracy = 0.27083334, global_step = 3, loss = 3.0553586 (0.266 sec)
accuracy = 0.390625, global_step = 4, loss = 2.9381728 (0.254 sec)
accuracy = 0.46354166, global_step = 5, loss = 3.023814 (0.257 sec)
accuracy = 0.5052083, global_step = 6, loss = 3.0399568 (0.252 sec)
accuracy = 0.5659722, global_step = 7, loss = 2.8596377 (0.255 sec)
accuracy = 0.6614583, global_step = 8, loss = 2.971867 (0.247 sec)
accuracy = 0.7621528, global_step = 9, loss = 2.9029965 (0.248 sec)
accuracy = 0.8454861, global_step = 10, loss = 2.9258032 (0.254 sec)
accuracy = 0.9774306, global_step = 11, loss = 2.873758 (0.259 sec)
accuracy = 1.140625, global_step = 12, loss = 2.80309 (0.264 sec)
accuracy = 1.25, global_step = 13, loss = 2.8447943 (0.257 sec)
accuracy = 1.3368056, global_step = 14, loss = 2.8756466 (0.273 sec)
accuracy = 1.4253472, global_step = 15, loss = 2.831986 (0.256 sec)
accuracy = 1.484375, global_step = 16, loss = 2.8789308 (0.257 sec)
accuracy = 1.5972222, global_step = 17, loss = 2.8527944 (0.266 sec)
accuracy = 1.7170138, global_step = 18, loss = 2.838802 (0.267 sec)
accuracy = 1.8368055, global_step = 19, loss = 2.784393 (0.249 sec)
accuracy = 1.9670138, global_step = 20, loss = 2.801551 (0.258 sec)
accuracy = 2.1267362, global_step = 21, loss = 2.8132067 (0.260 sec)
accuracy = 2.2291667, global_step = 22, loss = 2.809472 (0.243 sec)
accuracy = 2.4479167, global_step = 23, loss = 2.6915374 (0.257 sec)
accuracy = 2.4878473, global_step = 24, loss = 2.932784 (0.246 sec)
accuracy = 2.5694447, global_step = 25, loss = 2.8355052 (0.249 sec)
accuracy = 2.7187502, global_step = 26, loss = 2.8014967 (0.266 sec)
accuracy = 2.7864585, global_step = 27, loss = 2.8265033 (0.251 sec)
accuracy = 2.8767362, global_step = 28, loss = 2.8267145 (0.255 sec)
accuracy = 3.0954862, global_step = 29, loss = 2.6707861 (0.251 sec)
accuracy = 3.340278, global_step = 30, loss = 2.568421 (0.255 sec)
accuracy = 3.4687502, global_step = 31, loss = 2.8083658 (0.279 sec)
accuracy = 3.6250002, global_step = 32, loss = 2.6963851 (0.257 sec)
accuracy = 3.763889, global_step = 33, loss = 2.7898638 (0.251 sec)
accuracy = 3.9322917, global_step = 34, loss = 2.784389 (0.253 sec)
accuracy = 4.0364585, global_step = 35, loss = 2.8224938 (0.275 sec)
accuracy = 4.211806, global_step = 36, loss = 2.7806332 (0.258 sec)
accuracy = 4.3663197, global_step = 37, loss = 2.7826874 (0.247 sec)
accuracy = 4.595486, global_step = 38, loss = 2.7270017 (0.255 sec)
accuracy = 4.736111, global_step = 39, loss = 2.7599916 (0.264 sec)
accuracy = 4.8854165, global_step = 40, loss = 2.8012893 (0.247 sec)
accuracy = 5.076389, global_step = 41, loss = 2.7598324 (0.248 sec)
accuracy = 5.2604165, global_step = 42, loss = 2.7216122 (0.243 sec)
accuracy = 5.4184027, global_step = 43, loss = 2.7807002 (0.252 sec)
accuracy = 5.5190973, global_step = 44, loss = 2.8471205 (0.250 sec)
accuracy = 5.7152777, global_step = 45, loss = 2.7455971 (0.258 sec)
accuracy = 5.864583, global_step = 46, loss = 2.7729502 (0.257 sec)
accuracy = 6.0416665, global_step = 47, loss = 2.757571 (0.253 sec)
accuracy = 6.232639, global_step = 48, loss = 2.7516956 (0.242 sec)
accuracy = 6.40625, global_step = 49, loss = 2.733934 (0.256 sec)
accuracy = 6.5850697, global_step = 50, loss = 2.801941 (0.253 sec)
accuracy = 6.7482643, global_step = 51, loss = 2.7310755 (0.255 sec)
accuracy = 6.9513893, global_step = 52, loss = 2.7283907 (0.250 sec)
accuracy = 7.1406255, global_step = 53, loss = 2.756117 (0.263 sec)
accuracy = 7.30382, global_step = 54, loss = 2.7460463 (0.249 sec)
accuracy = 7.4635425, global_step = 55, loss = 2.766958 (0.259 sec)
accuracy = 7.592015, global_step = 56, loss = 2.768885 (0.255 sec)
accuracy = 7.7274313, global_step = 57, loss = 2.7649043 (0.255 sec)
accuracy = 7.9340286, global_step = 58, loss = 2.7583873 (0.253 sec)
accuracy = 8.050348, global_step = 59, loss = 2.7567563 (0.253 sec)
accuracy = 8.199654, global_step = 60, loss = 2.7471225 (0.255 sec)
accuracy = 8.423612, global_step = 61, loss = 2.7779036 (0.248 sec)
accuracy = 8.588542, global_step = 62, loss = 2.7876387 (0.249 sec)
accuracy = 8.782987, global_step = 63, loss = 2.726157 (0.253 sec)
global_step/sec: 3.70859
loss = 2.727338, step = 64 (17.258 sec)
accuracy = 9.006945, global_step = 64, loss = 2.727338 (0.260 sec)
accuracy = 9.121528, global_step = 65, loss = 2.7906888 (0.239 sec)
accuracy = 9.322917, global_step = 66, loss = 2.730347 (0.249 sec)
accuracy = 9.588542, global_step = 67, loss = 2.70981 (0.252 sec)
accuracy = 9.762153, global_step = 68, loss = 2.7340405 (0.243 sec)
accuracy = 9.944445, global_step = 69, loss = 2.7279184 (0.243 sec)
accuracy = 10.144097, global_step = 70, loss = 2.7155676 (0.258 sec)
accuracy = 10.328125, global_step = 71, loss = 2.6863642 (0.249 sec)
accuracy = 10.53993, global_step = 72, loss = 2.7186048 (0.255 sec)
accuracy = 10.71875, global_step = 73, loss = 2.5847907 (0.243 sec)
accuracy = 10.9375, global_step = 74, loss = 2.678436 (0.253 sec)
accuracy = 11.154514, global_step = 75, loss = 2.653829 (0.284 sec)
accuracy = 11.392362, global_step = 76, loss = 2.6233835 (0.260 sec)
accuracy = 11.526042, global_step = 77, loss = 2.7507424 (0.258 sec)
accuracy = 11.713542, global_step = 78, loss = 2.697833 (0.270 sec)
accuracy = 11.89757, global_step = 79, loss = 2.7222311 (0.259 sec)
accuracy = 12.027778, global_step = 80, loss = 2.6375299 (0.266 sec)
accuracy = 12.22743, global_step = 81, loss = 2.7068825 (0.268 sec)
accuracy = 12.524305, global_step = 82, loss = 2.7216706 (0.252 sec)
accuracy = 12.725695, global_step = 83, loss = 2.760762 (0.260 sec)
accuracy = 12.934028, global_step = 84, loss = 2.692858 (0.238 sec)
accuracy = 13.15625, global_step = 85, loss = 2.621553 (0.247 sec)
accuracy = 13.324653, global_step = 86, loss = 2.7130027 (0.278 sec)
accuracy = 13.472222, global_step = 87, loss = 2.7080705 (0.250 sec)
accuracy = 13.651042, global_step = 88, loss = 2.7245674 (0.243 sec)
accuracy = 13.831597, global_step = 89, loss = 2.6935027 (0.245 sec)
accuracy = 14.109375, global_step = 90, loss = 2.6721497 (0.277 sec)
accuracy = 14.286458, global_step = 91, loss = 2.7126837 (0.253 sec)
accuracy = 14.425347, global_step = 92, loss = 2.7609012 (0.260 sec)
accuracy = 14.595487, global_step = 93, loss = 2.7057211 (0.258 sec)
accuracy = 14.725695, global_step = 94, loss = 2.7015007 (0.254 sec)
accuracy = 14.901042, global_step = 95, loss = 2.7429445 (0.245 sec)
accuracy = 15.114584, global_step = 96, loss = 2.6857624 (0.244 sec)
accuracy = 15.312501, global_step = 97, loss = 2.6918457 (0.250 sec)
accuracy = 15.510418, global_step = 98, loss = 2.704358 (0.258 sec)
accuracy = 15.746529, global_step = 99, loss = 2.5907032 (0.259 sec)
accuracy = 15.984376, global_step = 100, loss = 2.683093 (0.259 sec)
accuracy = 16.182293, global_step = 101, loss = 2.7007818 (0.250 sec)
accuracy = 16.315973, global_step = 102, loss = 2.7258725 (0.251 sec)
accuracy = 16.583334, global_step = 103, loss = 2.6716244 (0.276 sec)
accuracy = 16.769098, global_step = 104, loss = 2.7110672 (0.244 sec)
accuracy = 16.907988, global_step = 105, loss = 2.738158 (0.264 sec)
accuracy = 17.232641, global_step = 106, loss = 2.6206734 (0.260 sec)
accuracy = 17.30903, global_step = 107, loss = 2.727392 (0.252 sec)
accuracy = 17.500002, global_step = 108, loss = 2.69766 (0.254 sec)
accuracy = 17.793406, global_step = 109, loss = 2.668313 (0.247 sec)
accuracy = 18.083336, global_step = 110, loss = 2.6034918 (0.265 sec)
accuracy = 18.2691, global_step = 111, loss = 2.6663983 (0.243 sec)
accuracy = 18.50521, global_step = 112, loss = 2.6910775 (0.256 sec)
accuracy = 18.682295, global_step = 113, loss = 2.6813397 (0.256 sec)
accuracy = 18.901045, global_step = 114, loss = 2.6816175 (0.257 sec)
accuracy = 19.171879, global_step = 115, loss = 2.6634688 (0.248 sec)
accuracy = 19.397573, global_step = 116, loss = 2.69096 (0.262 sec)
accuracy = 19.539934, global_step = 117, loss = 2.6889453 (0.243 sec)
accuracy = 19.758684, global_step = 118, loss = 2.6791432 (0.260 sec)
accuracy = 20.065975, global_step = 119, loss = 2.4584327 (0.256 sec)
accuracy = 20.19792, global_step = 120, loss = 2.691531 (0.264 sec)
accuracy = 20.506948, global_step = 121, loss = 2.5143275 (0.263 sec)
accuracy = 20.767365, global_step = 122, loss = 2.6538653 (0.258 sec)
accuracy = 21.003475, global_step = 123, loss = 2.6533415 (0.259 sec)
accuracy = 21.30903, global_step = 124, loss = 2.636662 (0.255 sec)
accuracy = 21.611115, global_step = 125, loss = 2.6333761 (0.259 sec)
accuracy = 21.864586, global_step = 126, loss = 2.6301222 (0.263 sec)
accuracy = 22.17535, global_step = 127, loss = 2.6063824 (0.246 sec)
global_step/sec: 3.91481
loss = 2.6788712, step = 128 (16.348 sec)
accuracy = 22.401045, global_step = 128, loss = 2.6788712 (0.258 sec)
accuracy = 22.616323, global_step = 129, loss = 2.6170273 (0.250 sec)
accuracy = 22.880213, global_step = 130, loss = 2.641676 (0.251 sec)
accuracy = 23.104172, global_step = 131, loss = 2.633234 (0.242 sec)
accuracy = 23.340282, global_step = 132, loss = 2.627761 (0.241 sec)
accuracy = 23.505213, global_step = 133, loss = 2.6438334 (0.249 sec)
accuracy = 23.732643, global_step = 134, loss = 2.6614769 (0.266 sec)
accuracy = 23.975698, global_step = 135, loss = 2.6480289 (0.248 sec)
accuracy = 24.244795, global_step = 136, loss = 2.5495558 (0.255 sec)
accuracy = 24.437504, global_step = 137, loss = 2.659622 (0.244 sec)
accuracy = 24.739588, global_step = 138, loss = 2.6182864 (0.262 sec)
accuracy = 24.986116, global_step = 139, loss = 2.646575 (0.267 sec)
accuracy = 25.180561, global_step = 140, loss = 2.6881626 (0.261 sec)
accuracy = 25.418407, global_step = 141, loss = 2.6619976 (0.283 sec)
accuracy = 25.696186, global_step = 142, loss = 2.6276755 (0.249 sec)
accuracy = 25.965282, global_step = 143, loss = 2.624519 (0.246 sec)
accuracy = 26.159727, global_step = 144, loss = 2.6261675 (0.245 sec)
accuracy = 26.447922, global_step = 145, loss = 2.6333718 (0.262 sec)
accuracy = 26.675352, global_step = 146, loss = 2.6342673 (0.243 sec)
accuracy = 26.989588, global_step = 147, loss = 2.62846 (0.254 sec)
accuracy = 27.295143, global_step = 148, loss = 2.55171 (0.263 sec)
accuracy = 27.55556, global_step = 149, loss = 2.6222396 (0.259 sec)
accuracy = 27.781254, global_step = 150, loss = 2.646287 (0.260 sec)
accuracy = 28.04167, global_step = 151, loss = 2.6066368 (0.248 sec)
accuracy = 28.177086, global_step = 152, loss = 2.6283333 (0.249 sec)
accuracy = 28.493057, global_step = 153, loss = 2.600973 (0.256 sec)
accuracy = 28.704863, global_step = 154, loss = 2.6244025 (0.244 sec)
accuracy = 28.996529, global_step = 155, loss = 2.6070163 (0.242 sec)
accuracy = 29.314238, global_step = 156, loss = 2.6002624 (0.252 sec)
accuracy = 29.548613, global_step = 157, loss = 2.608888 (0.255 sec)
accuracy = 29.812502, global_step = 158, loss = 2.6143763 (0.261 sec)
accuracy = 30.116322, global_step = 159, loss = 2.598233 (0.249 sec)
accuracy = 30.359377, global_step = 160, loss = 2.6417403 (0.265 sec)
accuracy = 30.631947, global_step = 161, loss = 2.6008775 (0.249 sec)
accuracy = 30.909725, global_step = 162, loss = 2.5977528 (0.261 sec)
accuracy = 31.196184, global_step = 163, loss = 2.599303 (0.264 sec)
accuracy = 31.432295, global_step = 164, loss = 2.600222 (0.248 sec)
accuracy = 31.6441, global_step = 165, loss = 2.6165211 (0.256 sec)
accuracy = 31.927086, global_step = 166, loss = 2.5532446 (0.245 sec)
accuracy = 32.26563, global_step = 167, loss = 2.597135 (0.246 sec)
accuracy = 32.583336, global_step = 168, loss = 2.5888078 (0.252 sec)
accuracy = 32.923615, global_step = 169, loss = 2.5773118 (0.256 sec)
accuracy = 33.225697, global_step = 170, loss = 2.5829735 (0.253 sec)
accuracy = 33.526043, global_step = 171, loss = 2.573926 (0.252 sec)
accuracy = 33.833336, global_step = 172, loss = 2.4220705 (0.279 sec)
accuracy = 34.168404, global_step = 173, loss = 2.5656028 (0.248 sec)
accuracy = 34.40278, global_step = 174, loss = 2.7774038 (0.277 sec)
accuracy = 34.602432, global_step = 175, loss = 2.5396516 (0.255 sec)
accuracy = 34.885418, global_step = 176, loss = 2.5774217 (0.262 sec)
accuracy = 35.23611, global_step = 177, loss = 2.5420485 (0.255 sec)
accuracy = 35.510418, global_step = 178, loss = 2.5823715 (0.244 sec)
accuracy = 35.819447, global_step = 179, loss = 2.5514119 (0.253 sec)
accuracy = 36.111115, global_step = 180, loss = 2.562154 (0.253 sec)
accuracy = 36.45313, global_step = 181, loss = 2.583521 (0.256 sec)
accuracy = 36.72396, global_step = 182, loss = 2.579455 (0.245 sec)
accuracy = 36.92882, global_step = 183, loss = 2.6181355 (0.247 sec)
accuracy = 37.250004, global_step = 184, loss = 2.5844162 (0.268 sec)
accuracy = 37.43924, global_step = 185, loss = 2.603065 (0.246 sec)
accuracy = 37.732643, global_step = 186, loss = 2.5676951 (0.258 sec)
accuracy = 37.98785, global_step = 187, loss = 2.5808156 (0.262 sec)
accuracy = 38.387157, global_step = 188, loss = 2.42301 (0.264 sec)
Saving checkpoints for 190 into ./sample/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-11-02T03:52:53Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./sample/model\model.ckpt-190
Running local_init_op.
Done running local_init_op.
Evaluation [6/66]
Evaluation [12/66]
Evaluation [18/66]
Evaluation [24/66]
Evaluation [30/66]
Evaluation [36/66]
Evaluation [42/66]
Evaluation [48/66]
Evaluation [54/66]
Evaluation [60/66]
Evaluation [66/66]
Finished evaluation at 2019-11-02-03:52:57
Saving dict for global step 190: accuracy = 0.203125, global_step = 190, loss = 2.5998418
Saving 'checkpoint_path' summary for global step 190: ./sample/model\model.ckpt-190
accuracy = 38.64063, global_step = 189, loss = 2.5831225 (11.053 sec)
Loss for final step: 2.5831225.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './sample/modeleval', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--train_on', '2015', '--epochs', '1', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/modeleval', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001E317E82828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001E317E832F0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:636: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./sample/modeleval\model.ckpt.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './sample/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--train_on', '2015', '--epochs', '1', '--pix250m', '384', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 22.0, '_save_checkpoints_steps': 22.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000020649E52908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002064B744268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 22.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:636: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./sample/model\model.ckpt-190
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './sample/model', '--datadir', './sample/data/24/multiple', '--train_on', '2015', '--epochs', '1', '--pix250m', '24', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 190.0, '_save_checkpoints_steps': 190.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001D712080C88>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001D70DDD4268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 190.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:636: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./sample/model\model.ckpt-190
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', './sample/modeleval', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--train_on', '2015', '--epochs', '1', '--convrnn_filters', '32', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/modeleval', '_tf_random_seed': None, '_save_summary_steps': 1.0, '_save_checkpoints_steps': 1.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002B5BEF33860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002B5BEF34268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:636: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', './sample/modeleval', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--train_on', '2015', '--epochs', '1', '--epochs', '1', '--pix250m', '24', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/modeleval', '_tf_random_seed': None, '_save_summary_steps': 22.0, '_save_checkpoints_steps': 22.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001C285792F60>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001C287034268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 22.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:636: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./sample/modeleval\model.ckpt.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', './sample/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--train_on', '2015', '--epochs', '1', '--epochs', '1', '--pix250m', '24', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 22.0, '_save_checkpoints_steps': 22.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000014562382828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000014563C852F0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 22.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', './sample/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--train_on', '2015', '--epochs', '1', '--epochs', '1', '--pix250m', '24', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 22.0, '_save_checkpoints_steps': 22.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001D30AB73F98>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001D30C4641E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 22.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', './sample/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--train_on', '2015', '--epochs', '1', '--epochs', '1', '--pix250m', '24', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 22.0, '_save_checkpoints_steps': 22.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000016F10F13828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000016F12805268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 22.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:636: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./sample/model\model.ckpt-190
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', './sample/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--train_on', '2015', '--epochs', '1', '--epochs', '1', '--pix250m', '384', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 22.0, '_save_checkpoints_steps': 22.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001D77DC327F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001D77F4E52F0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 22.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:636: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./sample/model\model.ckpt-190
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', './sample/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--train_on', '2015', '--epochs', '1', '--epochs', '1', '--pix250m', '384', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 22.0, '_save_checkpoints_steps': 22.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000024486052908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000024487985268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 22.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:636: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./sample/model\model.ckpt-190
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', './sample/model', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--train_on', '2015', '--epochs', '1', '--epochs', '1', '--pix250m', '384', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 22.0, '_save_checkpoints_steps': 22.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022DC57A3F98>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000022DC7063268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 22.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:636: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./sample/model\model.ckpt-190
Running local_init_op.
Done running local_init_op.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:92: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:92: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:92: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './sample/model', '--datadir', './sample/data/24/multiple', '--train_on', '2015', '--epochs', '1', '--pix250m', '24', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 190.0, '_save_checkpoints_steps': 190.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001FFA3A353C8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001FF9F8C31E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 190.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:185: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:152: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './sample/model', '--datadir', './sample/data/24/multiple', '--train_on', '2015', '--epochs', '1', '--pix250m', '24', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 190.0, '_save_checkpoints_steps': 190.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000021184CCCE48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000021180F321E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 190.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:185: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:152: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './sample/model', '--datadir', './sample/data/24/multiple', '--train_on', '2015', '--epochs', '1', '--pix250m', '24', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 190.0, '_save_checkpoints_steps': 190.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002B72E62DC88>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002B72A3821E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 190.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:187: Estimator.export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.
Instructions for updating:
This function has been renamed, use `export_saved_model` instead.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:151: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './sample/model', '--datadir', './sample/data/24/multiple', '--train_on', '2015', '--epochs', '1', '--pix250m', '24', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 190.0, '_save_checkpoints_steps': 190.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000027FF446A6A0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x0000027FF01B21E0>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 190.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:151: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './sample/model', '--datadir', './sample/data/24/multiple', '--train_on', '2015', '--epochs', '1', '--pix250m', '24', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 190.0, '_save_checkpoints_steps': 190.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002A21006A6A0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000002A20BD82268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 190.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './sample/model', '--datadir', './sample/data/24/multiple', '--train_on', '2015', '--epochs', '1', '--pix250m', '24', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 190.0, '_save_checkpoints_steps': 190.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001DEF20AC6A0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001DEEDE03268>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 190.0 or save_checkpoints_secs None.
Skipping training since max_steps has already saved.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:154: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:93: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:94: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:94: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:94: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.

From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\contrib\predictor\saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:93: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

TF_CONFIG environment variable: {'job': {'job_name': 'trainer.task', 'args': ['--modeldir', './sample/model', '--datadir', './sample/data/24/multiple', '--train_on', '2015', '--epochs', '1', '--pix250m', '24', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': './sample/model', '_tf_random_seed': None, '_save_summary_steps': 190.0, '_save_checkpoints_steps': 190.0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001BF6A40E518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Estimator's model_fn (<function _model_fn at 0x000001BF6615BC80>) includes params argument, but params are not passed to Estimator.
Not using Distribute Coordinator.
Running training and evaluation locally (non-distributed).
Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 190.0 or save_checkpoints_secs None.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:637: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Create CheckpointSaverHook.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Saving checkpoints for 0 into ./sample/model\model.ckpt.
loss = 3.0655043, step = 0
accuracy = 0.048611112, global_step = 0, loss = 3.0655043
accuracy = 0.097222224, global_step = 1, loss = 3.075771 (1.476 sec)
accuracy = 0.15625, global_step = 2, loss = 3.0612094 (0.291 sec)
accuracy = 0.20833333, global_step = 3, loss = 3.0888164 (0.319 sec)
accuracy = 0.2673611, global_step = 4, loss = 3.0612319 (0.298 sec)
accuracy = 0.3177083, global_step = 5, loss = 3.0709634 (0.323 sec)
accuracy = 0.39062497, global_step = 6, loss = 3.0500379 (0.301 sec)
accuracy = 0.44791663, global_step = 7, loss = 3.0680947 (0.295 sec)
accuracy = 0.51215273, global_step = 8, loss = 3.046966 (0.288 sec)
accuracy = 0.56597215, global_step = 9, loss = 3.078051 (0.284 sec)
accuracy = 0.62499994, global_step = 10, loss = 3.061052 (0.328 sec)
accuracy = 0.68576384, global_step = 11, loss = 3.0304 (0.323 sec)
accuracy = 0.75347215, global_step = 12, loss = 3.0652788 (0.304 sec)
accuracy = 0.80381936, global_step = 13, loss = 3.0920491 (0.290 sec)
accuracy = 0.8559027, global_step = 14, loss = 3.0430434 (0.306 sec)
accuracy = 0.89756936, global_step = 15, loss = 3.091776 (0.300 sec)
accuracy = 0.9357638, global_step = 16, loss = 3.0775313 (0.336 sec)
accuracy = 0.9982638, global_step = 17, loss = 3.049362 (0.291 sec)
accuracy = 1.0520833, global_step = 18, loss = 3.0307024 (0.307 sec)
accuracy = 1.1093749, global_step = 19, loss = 3.0672648 (0.305 sec)
accuracy = 1.1597221, global_step = 20, loss = 3.0672069 (0.311 sec)
accuracy = 1.2135415, global_step = 21, loss = 3.0505428 (0.299 sec)
accuracy = 1.2708331, global_step = 22, loss = 3.033595 (0.304 sec)
accuracy = 1.3281248, global_step = 23, loss = 3.0747914 (0.294 sec)
accuracy = 1.3854164, global_step = 24, loss = 3.0575588 (0.313 sec)
accuracy = 1.440972, global_step = 25, loss = 3.072167 (0.300 sec)
accuracy = 1.4947914, global_step = 26, loss = 3.0646393 (0.291 sec)
accuracy = 1.550347, global_step = 27, loss = 3.0445428 (0.324 sec)
accuracy = 1.612847, global_step = 28, loss = 3.0539765 (0.304 sec)
accuracy = 1.6770831, global_step = 29, loss = 3.0349011 (0.326 sec)
accuracy = 1.7517359, global_step = 30, loss = 3.039522 (0.298 sec)
accuracy = 1.8194443, global_step = 31, loss = 3.051874 (0.298 sec)
accuracy = 1.8854165, global_step = 32, loss = 3.0234923 (0.309 sec)
accuracy = 1.9496527, global_step = 33, loss = 3.0370276 (0.323 sec)
accuracy = 1.9947915, global_step = 34, loss = 3.0405674 (0.312 sec)
accuracy = 2.0624998, global_step = 35, loss = 3.032521 (0.287 sec)
accuracy = 2.128472, global_step = 36, loss = 3.0302305 (0.326 sec)
accuracy = 2.1944444, global_step = 37, loss = 3.0475085 (0.321 sec)
accuracy = 2.2829862, global_step = 38, loss = 2.983274 (0.298 sec)
accuracy = 2.3506944, global_step = 39, loss = 2.9524143 (0.337 sec)
accuracy = 2.4322917, global_step = 40, loss = 2.9926558 (0.300 sec)
accuracy = 2.5243056, global_step = 41, loss = 2.9301724 (0.302 sec)
accuracy = 2.6128473, global_step = 42, loss = 3.0585835 (0.310 sec)
accuracy = 2.6875, global_step = 43, loss = 3.0132465 (0.311 sec)
accuracy = 2.7430556, global_step = 44, loss = 2.9965417 (0.333 sec)
accuracy = 2.8107638, global_step = 45, loss = 2.9573927 (0.296 sec)
accuracy = 2.859375, global_step = 46, loss = 3.103896 (0.308 sec)
accuracy = 2.9392362, global_step = 47, loss = 2.9604309 (0.284 sec)
accuracy = 2.9947917, global_step = 48, loss = 2.922107 (0.315 sec)
accuracy = 3.0520835, global_step = 49, loss = 2.943875 (0.296 sec)
accuracy = 3.107639, global_step = 50, loss = 2.8864963 (0.307 sec)
accuracy = 3.217014, global_step = 51, loss = 2.8618045 (0.280 sec)
accuracy = 3.2864585, global_step = 52, loss = 2.784693 (0.326 sec)
accuracy = 3.3506947, global_step = 53, loss = 2.8987067 (0.285 sec)
accuracy = 3.4114585, global_step = 54, loss = 2.9040818 (0.289 sec)
accuracy = 3.4704862, global_step = 55, loss = 2.9300196 (0.338 sec)
accuracy = 3.53125, global_step = 56, loss = 2.8623188 (0.316 sec)
accuracy = 3.5451388, global_step = 57, loss = 3.0120845 (0.370 sec)
accuracy = 3.612847, global_step = 58, loss = 2.8675356 (0.357 sec)
accuracy = 3.7604165, global_step = 59, loss = 2.6696723 (0.367 sec)
accuracy = 3.8211803, global_step = 60, loss = 2.8582084 (0.320 sec)
accuracy = 3.9479165, global_step = 61, loss = 2.6921391 (0.321 sec)
accuracy = 4.0625, global_step = 62, loss = 2.6479354 (0.331 sec)
accuracy = 4.123264, global_step = 63, loss = 2.938352 (0.321 sec)
global_step/sec: 3.03472
loss = 2.8172333, step = 64 (21.092 sec)
accuracy = 4.2239585, global_step = 64, loss = 2.8172333 (0.368 sec)
accuracy = 4.3177085, global_step = 65, loss = 2.8242211 (0.337 sec)
accuracy = 4.46875, global_step = 66, loss = 2.6563728 (0.360 sec)
accuracy = 4.545139, global_step = 67, loss = 2.7615237 (0.362 sec)
accuracy = 4.663194, global_step = 68, loss = 2.829358 (0.347 sec)
accuracy = 4.8142357, global_step = 69, loss = 2.7355614 (0.352 sec)
accuracy = 4.956597, global_step = 70, loss = 2.7114754 (0.291 sec)
accuracy = 5.1579857, global_step = 71, loss = 2.5872502 (0.407 sec)
accuracy = 5.241319, global_step = 72, loss = 2.8177028 (0.323 sec)
accuracy = 5.5243053, global_step = 73, loss = 2.57324 (0.358 sec)
accuracy = 5.631944, global_step = 74, loss = 2.7770267 (0.411 sec)
accuracy = 5.864583, global_step = 75, loss = 2.4906168 (0.294 sec)
accuracy = 6.0399303, global_step = 76, loss = 2.7744322 (0.348 sec)
accuracy = 6.1892357, global_step = 77, loss = 2.8827543 (0.402 sec)
accuracy = 6.489583, global_step = 78, loss = 2.5708704 (0.321 sec)
accuracy = 6.6406245, global_step = 79, loss = 2.8049235 (0.334 sec)
accuracy = 6.9548607, global_step = 80, loss = 2.4978151 (0.353 sec)
accuracy = 7.151041, global_step = 81, loss = 2.6223986 (0.306 sec)
accuracy = 7.41493, global_step = 82, loss = 2.5825956 (0.311 sec)
accuracy = 7.680555, global_step = 83, loss = 2.5697966 (0.332 sec)
accuracy = 7.8802075, global_step = 84, loss = 2.6548038 (0.360 sec)
accuracy = 8.152777, global_step = 85, loss = 2.49508 (0.342 sec)
accuracy = 8.482637, global_step = 86, loss = 2.5204573 (0.313 sec)
accuracy = 8.763887, global_step = 87, loss = 2.612463 (0.332 sec)
accuracy = 9.163193, global_step = 88, loss = 2.5872784 (0.332 sec)
accuracy = 9.411457, global_step = 89, loss = 2.5124714 (0.373 sec)
accuracy = 9.512152, global_step = 90, loss = 2.8552516 (0.322 sec)
accuracy = 9.822916, global_step = 91, loss = 2.4820426 (0.309 sec)
accuracy = 10.022569, global_step = 92, loss = 2.7257864 (0.335 sec)
accuracy = 10.357638, global_step = 93, loss = 2.456481 (0.332 sec)
accuracy = 10.729166, global_step = 94, loss = 2.467515 (0.305 sec)
accuracy = 11.152777, global_step = 95, loss = 2.392052 (0.341 sec)
accuracy = 11.446179, global_step = 96, loss = 2.5251334 (0.303 sec)
accuracy = 11.690971, global_step = 97, loss = 2.7167945 (0.333 sec)
accuracy = 11.944444, global_step = 98, loss = 2.6396787 (0.356 sec)
accuracy = 12.314236, global_step = 99, loss = 2.4743183 (0.307 sec)
accuracy = 12.565971, global_step = 100, loss = 2.6885972 (0.308 sec)
accuracy = 12.885416, global_step = 101, loss = 2.6565726 (0.296 sec)
accuracy = 13.079861, global_step = 102, loss = 2.6373441 (0.335 sec)
accuracy = 13.611111, global_step = 103, loss = 2.324136 (0.317 sec)
accuracy = 14.07118, global_step = 104, loss = 2.3843882 (0.287 sec)
accuracy = 14.50868, global_step = 105, loss = 2.3647146 (0.336 sec)
accuracy = 14.942708, global_step = 106, loss = 2.427057 (0.323 sec)
accuracy = 15.069444, global_step = 107, loss = 2.8417752 (0.294 sec)
accuracy = 15.317708, global_step = 108, loss = 2.6509123 (0.354 sec)
accuracy = 15.612847, global_step = 109, loss = 2.586025 (0.297 sec)
accuracy = 16.03125, global_step = 110, loss = 2.4171057 (0.316 sec)
accuracy = 16.401041, global_step = 111, loss = 2.469232 (0.308 sec)
accuracy = 16.822916, global_step = 112, loss = 2.4537237 (0.293 sec)
accuracy = 17.010416, global_step = 113, loss = 2.7853038 (0.291 sec)
accuracy = 17.243055, global_step = 114, loss = 2.6999178 (0.315 sec)
accuracy = 17.559027, global_step = 115, loss = 2.66117 (0.298 sec)
accuracy = 17.838541, global_step = 116, loss = 2.641239 (0.280 sec)
accuracy = 18.23611, global_step = 117, loss = 2.4808893 (0.310 sec)
accuracy = 18.61111, global_step = 118, loss = 2.567598 (0.289 sec)
accuracy = 19.098957, global_step = 119, loss = 2.3966703 (0.304 sec)
accuracy = 19.352428, global_step = 120, loss = 2.676315 (0.319 sec)
accuracy = 19.758678, global_step = 121, loss = 2.504764 (0.314 sec)
accuracy = 20.243053, global_step = 122, loss = 2.362511 (0.316 sec)
accuracy = 20.795137, global_step = 123, loss = 2.2886264 (0.289 sec)
accuracy = 21.295137, global_step = 124, loss = 2.4279687 (0.266 sec)
accuracy = 21.741318, global_step = 125, loss = 2.435626 (0.277 sec)
accuracy = 22.111109, global_step = 126, loss = 2.5677369 (0.296 sec)
accuracy = 22.55729, global_step = 127, loss = 2.4067385 (0.312 sec)
global_step/sec: 3.09624
loss = 2.426122, step = 128 (20.668 sec)
accuracy = 23.006943, global_step = 128, loss = 2.426122 (0.284 sec)
accuracy = 23.52604, global_step = 129, loss = 2.2684639 (0.308 sec)
accuracy = 24.069443, global_step = 130, loss = 2.3708043 (0.310 sec)
accuracy = 24.501734, global_step = 131, loss = 2.423211 (0.339 sec)
accuracy = 24.894094, global_step = 132, loss = 2.5131917 (0.285 sec)
accuracy = 25.50347, global_step = 133, loss = 2.2418306 (0.280 sec)
accuracy = 25.883678, global_step = 134, loss = 2.5328548 (0.297 sec)
accuracy = 26.430553, global_step = 135, loss = 2.3836737 (0.311 sec)
accuracy = 26.796873, global_step = 136, loss = 2.6530142 (0.295 sec)
accuracy = 27.281248, global_step = 137, loss = 2.3494105 (0.312 sec)
accuracy = 27.828123, global_step = 138, loss = 2.3492804 (0.314 sec)
accuracy = 28.269094, global_step = 139, loss = 2.4394424 (0.280 sec)
accuracy = 28.800344, global_step = 140, loss = 2.34421 (0.302 sec)
accuracy = 29.32465, global_step = 141, loss = 2.3538878 (0.266 sec)
accuracy = 29.822914, global_step = 142, loss = 2.426147 (0.331 sec)
accuracy = 30.21354, global_step = 143, loss = 2.4177475 (0.300 sec)
accuracy = 30.678818, global_step = 144, loss = 2.5483778 (0.298 sec)
accuracy = 31.032984, global_step = 145, loss = 2.6765275 (0.329 sec)
accuracy = 31.461803, global_step = 146, loss = 2.600212 (0.299 sec)
accuracy = 32.055553, global_step = 147, loss = 2.2850673 (0.272 sec)
accuracy = 32.420135, global_step = 148, loss = 2.5160785 (0.293 sec)
accuracy = 32.861107, global_step = 149, loss = 2.3683376 (0.319 sec)
accuracy = 33.409718, global_step = 150, loss = 2.4701803 (0.296 sec)
accuracy = 33.991314, global_step = 151, loss = 2.2806978 (0.297 sec)
accuracy = 34.435757, global_step = 152, loss = 2.4853122 (0.290 sec)
accuracy = 34.977425, global_step = 153, loss = 2.3602471 (0.349 sec)
accuracy = 35.213535, global_step = 154, loss = 2.6550667 (0.307 sec)
accuracy = 35.842007, global_step = 155, loss = 2.22499 (0.280 sec)
accuracy = 36.236103, global_step = 156, loss = 2.404648 (0.275 sec)
accuracy = 36.564228, global_step = 157, loss = 2.6619492 (0.307 sec)
accuracy = 37.03992, global_step = 158, loss = 2.3430629 (0.298 sec)
accuracy = 37.305546, global_step = 159, loss = 2.5913374 (0.300 sec)
accuracy = 37.722214, global_step = 160, loss = 2.424348 (0.316 sec)
accuracy = 38.126728, global_step = 161, loss = 2.471074 (0.315 sec)
accuracy = 38.65277, global_step = 162, loss = 2.3078723 (0.294 sec)
accuracy = 39.182285, global_step = 163, loss = 2.317204 (0.280 sec)
accuracy = 39.28819, global_step = 164, loss = 2.7762327 (0.284 sec)
accuracy = 39.491314, global_step = 165, loss = 2.7618425 (0.288 sec)
accuracy = 40.17708, global_step = 166, loss = 2.1579971 (0.281 sec)
accuracy = 40.72048, global_step = 167, loss = 2.2700377 (0.338 sec)
accuracy = 41.25694, global_step = 168, loss = 2.393836 (0.302 sec)
accuracy = 41.71527, global_step = 169, loss = 2.3282309 (0.309 sec)
accuracy = 42.29513, global_step = 170, loss = 2.3100502 (0.339 sec)
accuracy = 43.017353, global_step = 171, loss = 2.2211866 (0.286 sec)
accuracy = 43.565964, global_step = 172, loss = 2.3694327 (0.313 sec)
accuracy = 44.187492, global_step = 173, loss = 2.1987524 (0.322 sec)
accuracy = 44.749992, global_step = 174, loss = 2.313918 (0.286 sec)
accuracy = 45.218742, global_step = 175, loss = 2.4347792 (0.284 sec)
accuracy = 45.624992, global_step = 176, loss = 2.5722134 (0.286 sec)
accuracy = 46.128464, global_step = 177, loss = 2.3529434 (0.274 sec)
accuracy = 46.68228, global_step = 178, loss = 2.3196564 (0.295 sec)
accuracy = 47.149296, global_step = 179, loss = 2.3701723 (0.306 sec)
accuracy = 47.46874, global_step = 180, loss = 2.5749938 (0.333 sec)
accuracy = 48.067696, global_step = 181, loss = 2.2749398 (0.330 sec)
accuracy = 48.604153, global_step = 182, loss = 2.3929415 (0.299 sec)
accuracy = 49.098946, global_step = 183, loss = 2.4373407 (0.282 sec)
accuracy = 49.717003, global_step = 184, loss = 2.2283423 (0.323 sec)
accuracy = 50.260406, global_step = 185, loss = 2.3347208 (0.345 sec)
accuracy = 50.81249, global_step = 186, loss = 2.2695873 (0.282 sec)
accuracy = 51.461796, global_step = 187, loss = 2.1722288 (0.329 sec)
accuracy = 51.99478, global_step = 188, loss = 2.4295723 (0.297 sec)
Saving checkpoints for 190 into ./sample/model\model.ckpt.
Calling model_fn.
Done calling model_fn.
Starting evaluation at 2019-12-22T12:07:23Z
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from ./sample/model\model.ckpt-190
Running local_init_op.
Done running local_init_op.
Evaluation [6/66]
Evaluation [12/66]
Evaluation [18/66]
Evaluation [24/66]
Evaluation [30/66]
Evaluation [36/66]
Evaluation [42/66]
Evaluation [48/66]
Evaluation [54/66]
Evaluation [60/66]
Evaluation [66/66]
Finished evaluation at 2019-12-22-12:07:28
Saving dict for global step 190: accuracy = 0.42708334, global_step = 190, loss = 2.493431
Saving 'checkpoint_path' summary for global step 190: ./sample/model\model.ckpt-190
accuracy = 52.583324, global_step = 189, loss = 2.374533 (12.502 sec)
Loss for final step: 2.374533.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\task.py:154: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:92: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:93: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:93: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:93: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.

From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\contrib\predictor\saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:92: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:93: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:93: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:93: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.

From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\contrib\predictor\saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--epochs', '10', '--pix250m', '384', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--epochs', '10', '--pix250m', '384', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/AMZ/models/2_gcloud/1\\model.ckpt-14160', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000019F61A89550>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Could not find trained model in model_dir: E:/acocac/research/AMZ/models/2_gcloud/1\model.ckpt-14160, running initialization to predict.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--epochs', '10', '--pix250m', '384', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/AMZ/models/2_gcloud/1\\model.ckpt-14160', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002CB65A39550>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Could not find trained model in model_dir: E:/acocac/research/AMZ/models/2_gcloud/1\model.ckpt-14160, running initialization to predict.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:500: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--epochs', '10', '--pix250m', '384', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/AMZ/models/2_gcloud/1\\model.ckpt-14160', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001AC1DE49550>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Could not find trained model in model_dir: E:/acocac/research/AMZ/models/2_gcloud/1\model.ckpt-14160, running initialization to predict.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:500: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--epochs', '10', '--pix250m', '384', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1', '--limit_batches', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/AMZ/models/2_gcloud/1\\model.ckpt-14160', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001CCE32F7550>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Could not find trained model in model_dir: E:/acocac/research/AMZ/models/2_gcloud/1\model.ckpt-14160, running initialization to predict.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:500: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--epochs', '10', '--pix250m', '384', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/AMZ/models/2_gcloud/1\\model.ckpt-14160', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002B40B3A9550>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Could not find trained model in model_dir: E:/acocac/research/AMZ/models/2_gcloud/1\model.ckpt-14160, running initialization to predict.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:500: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--epochs', '10', '--pix250m', '384', '--convrnn_filters', '32', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/AMZ/models/2_gcloud/1\\model.ckpt-14160', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001704B6C1C18>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:92: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:93: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:93: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:93: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.

From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\contrib\predictor\saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:92: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:93: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:93: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:93: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.

From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\contrib\predictor\saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--reference', 'MCD12Q1v6stable_LCProp2', '--epochs', '10', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/AMZ/models/2_gcloud/1\\model.ckpt-14160', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002D1325A0DA0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Could not find trained model in model_dir: E:/acocac/research/AMZ/models/2_gcloud/1\model.ckpt-14160, running initialization to predict.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:500: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--reference', 'MCD12Q1v6stable_LCProp2', '--epochs', '10', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/AMZ/models/2_gcloud/1\\model.ckpt-14160', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001E77DD3FE80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Could not find trained model in model_dir: E:/acocac/research/AMZ/models/2_gcloud/1\model.ckpt-14160, running initialization to predict.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:500: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
Could not find trained model in model_dir: E:/acocac/research/AMZ/models/2_gcloud/1\model.ckpt-14160, running initialization to predict.
Calling model_fn.
Done calling model_fn.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--reference', 'MCD12Q1v6stable_LCProp2', '--epochs', '10', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/AMZ/models/2_gcloud/1\\model.ckpt-14160', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000018A4BE11CC0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Could not find trained model in model_dir: E:/acocac/research/AMZ/models/2_gcloud/1\model.ckpt-14160, running initialization to predict.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--reference', 'MCD12Q1v6stable_LCProp2', '--epochs', '10', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/AMZ/models/2_gcloud/1\\model.ckpt-14160', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001D8A8CD0CC0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Could not find trained model in model_dir: E:/acocac/research/AMZ/models/2_gcloud/1\model.ckpt-14160, running initialization to predict.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:500: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--reference', 'MCD12Q1v6stable_LCProp2', '--epochs', '10', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/AMZ/models/2_gcloud/1\\model.ckpt-14160', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002AFC5840E80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Could not find trained model in model_dir: E:/acocac/research/AMZ/models/2_gcloud/1\model.ckpt-14160, running initialization to predict.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:500: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--reference', 'MCD12Q1v6stable_LCProp2', '--epochs', '10', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/AMZ/models/2_gcloud/1\\model.ckpt-14160', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002C62AB40E80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Could not find trained model in model_dir: E:/acocac/research/AMZ/models/2_gcloud/1\model.ckpt-14160, running initialization to predict.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:500: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--reference', 'MCD12Q1v6stable_LCProp2', '--epochs', '10', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/AMZ/models/2_gcloud/1\\model.ckpt-14160', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000258528E1E48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Could not find trained model in model_dir: E:/acocac/research/AMZ/models/2_gcloud/1\model.ckpt-14160, running initialization to predict.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--reference', 'MCD12Q1v6stable_LCProp2', '--epochs', '10', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/AMZ/models/2_gcloud/1\\model.ckpt-14160', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000290186F2E48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Could not find trained model in model_dir: E:/acocac/research/AMZ/models/2_gcloud/1\model.ckpt-14160, running initialization to predict.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--reference', 'MCD12Q1v6stable_LCProp2', '--epochs', '10', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/AMZ/models/2_gcloud/1\\model.ckpt-14160', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001A1EC031E48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Could not find trained model in model_dir: E:/acocac/research/AMZ/models/2_gcloud/1\model.ckpt-14160, running initialization to predict.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--reference', 'MCD12Q1v6stable_LCProp2', '--epochs', '10', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/AMZ/models/2_gcloud/1\\model.ckpt-14160', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000020F89E41E80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Could not find trained model in model_dir: E:/acocac/research/AMZ/models/2_gcloud/1\model.ckpt-14160, running initialization to predict.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:497: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:397: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.

Done calling model_fn.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--reference', 'MCD12Q1v6stable_LCProp2', '--epochs', '10', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/AMZ/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000020E4CE70E48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:496: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/AMZ/models/2_gcloud/1\model.ckpt-14160
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--reference', 'MCD12Q1v6stable_LCProp2', '--epochs', '10', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/AMZ/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001AD26140E48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:496: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/AMZ/models/2_gcloud/1\model.ckpt-14160
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--reference', 'MCD12Q1v6stable_LCProp2', '--epochs', '10', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/AMZ/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001CE4BE20C18>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:496: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/AMZ/models/2_gcloud/1\model.ckpt-14160
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/AMZ/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/AMZ/MOD09_250m500m/gz/384/multiple', '--storedir', 'E:/acocac/research/AMZ/eval/pred/2_gcloud/1', '--train_on', '2015', '--reference', 'MCD12Q1v6stable_LCProp2', '--epochs', '10', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/AMZ/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000026FB05F1F28>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:496: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/AMZ/models/2_gcloud/1\model.ckpt-14160
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--train_on', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002AAFAD51588>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--train_on', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F2924F2588>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:496: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/1\model.ckpt-119
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--train_on', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001A387C453C8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--train_on', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F984C03278>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:505: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/1\model.ckpt-119
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--train_on', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001AAFD015240>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--train_on', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000024BA4EDF9B0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000020F12FBEC18>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001EE092BEBE0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:505: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/1\model.ckpt-119
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F0358BFCC0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:505: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/1\model.ckpt-119
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000211446EEEB8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:507: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/1\model.ckpt-119
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000017A46631400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:507: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/1\model.ckpt-119
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001FE66DF2F60>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:507: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/1\model.ckpt-119
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002017D682160>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:507: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/1\model.ckpt-119
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001EB50AF2F98>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:507: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/1\model.ckpt-119
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001FA0CCC2F98>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:507: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/1\model.ckpt-119
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002325CEB9978>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:507: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/1\model.ckpt-119
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002C517751F98>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Could not find trained model in model_dir: E:/acocac/research/tile_0_563/models/2_gcloud/1, running initialization to predict.
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:507: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000255A4291F98>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:507: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/1\model.ckpt-119
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001AE311E5CC0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:507: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001AE31246AC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001AE31246AC8>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001AE31246AC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001AE31246AC8>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/1\model.ckpt-119
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001C474C45E80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:507: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001C474CA7A20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001C474CA7A20>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001C474CA7A20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001C474CA7A20>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/1\model.ckpt-119
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F1370B5E80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:507: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001F137116A20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001F137116A20>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001F137116A20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001F137116A20>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/1\model.ckpt-119
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/24/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '24', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000014C5EFA5E80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:507: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x0000014C5F008AC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x0000014C5F008AC8>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x0000014C5F008AC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x0000014C5F008AC8>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/1\model.ckpt-119
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001B55EB09CC0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:507: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001B55EB56588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001B55EB56588>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001B55EB56588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001B55EB56588>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/1\model.ckpt-119
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002BDDCABBEB8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:508: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000002BDDCB075F8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000002BDDCB075F8>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000002BDDCB075F8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000002BDDCB075F8>>: AttributeError: module 'gast' has no attribute 'Num'
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002950EE59D30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:508: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000002950EEA75F8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000002950EEA75F8>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000002950EEA75F8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000002950EEA75F8>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/1\model.ckpt-119
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/1', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001D2FC869CC0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:508: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001D2FC8B6518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001D2FC8B6518>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001D2FC8B6518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001D2FC8B6518>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/1\model.ckpt-119
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2001-2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2001-2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2001-2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001B9B2C59CC0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:508: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001B9B2CA54E0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001B9B2CA54E0>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001B9B2CA54E0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001B9B2CA54E0>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/t2001-2003_1to18\model.ckpt-1069
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2001-2003_1to3', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2001-2003_1to3/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2001-2003_1to3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002520A449CF8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:508: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000002520A495518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000002520A495518>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000002520A495518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000002520A495518>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/t2001-2003_1to3\model.ckpt-1069
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to3', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to3/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001EFA1CF9CC0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:508: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001EFA1D46518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001EFA1D46518>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001EFA1D46518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001EFA1D46518>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to3\model.ckpt-357
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001B164779CC0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:508: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001B1647C54E0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001B1647C54E0>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001B1647C54E0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001B1647C54E0>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18\model.ckpt-357
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F9D980BEB8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001E41697BEB8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000018799C7ACF8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001EB7D009DD8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:94: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:95: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:95: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.

From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\eval2.py:95: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.

From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\contrib\predictor\saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--step', 'evaluation', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000163B9C66400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--step', 'evaluation', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022DE205BF98>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--step', 'evaluation', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001B52DA95438>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:510: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001B52DDA9198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001B52DDA9198>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001B52DDA9198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001B52DDA9198>>: AttributeError: module 'gast' has no attribute 'Num'
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--step', 'evaluation', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022BA6995438>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:508: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x0000022BA6CA9710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x0000022BA6CA9710>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x0000022BA6CA9710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x0000022BA6CA9710>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18\model.ckpt-357
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--step', 'evaluation', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000271EB7B9F98>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:508: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x00000271EB807668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x00000271EB807668>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x00000271EB807668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x00000271EB807668>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18\model.ckpt-357
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000179C1EB9EB8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:508: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x00000179C1F070B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x00000179C1F070B8>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x00000179C1F070B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x00000179C1F070B8>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18\model.ckpt-357
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001C1BCEC9CF8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:508: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001C1BCF16518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001C1BCF16518>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001C1BCF16518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001C1BCF16518>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18\model.ckpt-357
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001D19B859FD0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:508: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001D19B8A6550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001D19B8A6550>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001D19B8A6550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001D19B8A6550>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18\model.ckpt-357
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000025419FE9FD0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:508: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000002541A035518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000002541A035518>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000002541A035518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000002541A035518>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18\model.ckpt-357
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000191EF659DD8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:508: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x00000191EF322630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x00000191EF322630>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x00000191EF322630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x00000191EF322630>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18\model.ckpt-357
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000253E2C19B70>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:508: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x00000253E28DD048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x00000253E28DD048>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x00000253E28DD048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x00000253E28DD048>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18\model.ckpt-357
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001BEE3279DD8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:508: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001BEE32C7080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001BEE32C7080>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001BEE32C7080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001BEE32C7080>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18\model.ckpt-357
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000017657C86DD8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001193C138DD8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001676CDE8C88>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--step', 'evaluation', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F2B8C4BE10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--step', 'evaluation', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000021CE96D9DA0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--step', 'evaluation', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023FE9D09C18>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--step', 'evaluation', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F4D1AD8EB8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--step', 'evaluation', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000019C2F628E10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--step', 'evaluation', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000209899760F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:508: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x0000020989C72160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x0000020989C72160>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x0000020989C72160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x0000020989C72160>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18\model.ckpt-357
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--step', 'evaluation', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002391E1450F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:508: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000002391E443160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000002391E443160>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000002391E443160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000002391E443160>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18\model.ckpt-357
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--step', 'evaluation', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001E4E2449F60>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--step', 'evaluation', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001D0D19787F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:501: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001D0D1C92160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001D0D1C92160>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001D0D1C92160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001D0D1C92160>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18\model.ckpt-357
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--step', 'evaluation', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001D792365550>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:502: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001D792968EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001D792968EB8>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001D792968EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x000001D792968EB8>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18\model.ckpt-357
Running local_init_op.
Done running local_init_op.
TF_CONFIG environment variable: {'job': {'job_name': 'trainer.eval', 'args': ['--modeldir', 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '--datadir', 'F:/acoca/research/gee/dataset/tile_0_563/gz/384/multiple', '--storedir', 'E:/acocac/research/tile_0_563/eval/pred/2_gcloud/t2003_1to18/2001', '--dataset', '2001', '--reference', 'MCD12Q1v6stable01to18_LCProp2', '--epochs', '20', '--pix250m', '384', '--convrnn_filters', '64', '--convrnn_layers', '1', '--learning_rate', '0.01', '--writetiles', '--writeconfidences', '--step', 'evaluation', '--batchsize', '1']}, 'task': {}, 'cluster': {}, 'environment': 'cloud'}
Using config: {'_model_dir': 'E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 1, '_log_step_count_steps': 64, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000018C911E5400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
Calling model_fn.
From E:\acocac\research\scripts\thesis_cloud\3c_model_mtlcc_estimator\trainer\model.py:502: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x0000018C914F7128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x0000018C914F7128>>: AttributeError: module 'gast' has no attribute 'Num'
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\ops\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Entity <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x0000018C914F7128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvGRUCell.call of <trainer.utils.ConvGRUCell object at 0x0000018C914F7128>>: AttributeError: module 'gast' has no attribute 'Num'
Done calling model_fn.
Graph was finalized.
From C:\Users\PAPAZ\AppData\Local\Continuum\anaconda2\envs\tf_gpu_v114\lib\site-packages\tensorflow\python\training\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
Restoring parameters from E:/acocac/research/tile_0_563/models/2_gcloud/t2003_1to18\model.ckpt-357
Running local_init_op.
Done running local_init_op.
