"""
Write reference GRID of TFrecords exported from GEE

Example invocation::

    python 2_datapartition/0_write_patchfileid_geotiff.py
        -r /home/xx/
        -y 2009
        -p 46

acocac@gmail.com
"""

import tensorflow as tf
import numpy as np
import os
import glob
import rasterio
from rasterio.transform import Affine
import json
import argparse

parser = argparse.ArgumentParser(description='Export gee data to visualise in the GEE code editor')

parser.add_argument('-r','--rootdir', type=str, required=True, help='Dir with input TFrecords.gz generated by GEE')
parser.add_argument('-y','--tyear', type=str, required=True, help='Target year')
parser.add_argument('-p','--psize', type=int, required=True, help='patch size value set of the MODIS 250-m data')

def write_tif(filename, Z, crs, affine):
    kwargs = {
        'crs': {'init': crs},
        'transform': Affine(affine[0], affine[1], affine[2], affine[3], affine[4], affine[5]),
        'count': Z.shape[0],
        'dtype': rasterio.int32,
        'precision': 12,
        'driver': 'GTiff',
        'width': Z.shape[1],
        'height': Z.shape[2],
        'nodata': -9999
    }

    print(affine[0], affine[1], affine[2], affine[3], affine[4], affine[5])
    with rasterio.open(filename, 'w', **kwargs) as dst:
        dst.write(Z)

def merge_tfrecord(patch_dict, patchesPerRow):

    M = patch_dict
    d1 = []  # will be used to concatenate
    X = []

    numberOfIterations = M.__len__()//patchesPerRow

    for i in range(0, numberOfIterations):
        for j in range(0, patchesPerRow):
            d1.append(M[j + patchesPerRow * i])
        d1 = np.concatenate(d1, 1)
        X.append(d1)
        d1 = []

    allarr = np.concatenate(X, axis=0)
    y = np.expand_dims(allarr, axis=0)

    return(y)

def array1d(n_patches, psize):
    id_all = []
    for i in range(n_patches):
        id_temp = np.full((psize, psize), i)
        id_all.append(id_temp)
    return(id_all)

def reshape_array(x, psize):
    return x.reshape(psize,psize)

if __name__ == '__main__':
    args = parser.parse_args()
    rootdir = args.rootdir
    tyear = args.tyear
    psize = args.psize

    outdir = os.path.join(rootdir, 'geodata','split', str(psize), 'raw', 'geoTIFF')
    if not os.path.exists(outdir):
        os.makedirs(outdir)

    options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)

    fileNames_250m = sorted(glob.glob(os.path.join(rootdir,'raw',str(psize),'data' + tyear[2:],'*.gz')),key=os.path.getctime)
    json_250m = sorted(glob.glob(os.path.join(rootdir,'raw',str(psize),'data' + tyear[2:],'*.json')),key=os.path.getctime)

    with open(json_250m[0], 'r') as f:
        mixjson = json.load(f)

    crs = mixjson['projection']['crs']
    affine = mixjson['projection']['affine']['doubleMatrix']
    patchesPerRow = mixjson['patchesPerRow']
    totalPatches_json = mixjson['totalPatches']

    if fileNames_250m.__len__() > 1:
        n_patches_first = sum(1 for _ in tf.python_io.tf_record_iterator(fileNames_250m[0], options=options))
        n_patches_last = sum(1 for _ in tf.python_io.tf_record_iterator(fileNames_250m[-1], options=options))

        totalPatches = (n_patches_first * (len(fileNames_250m)-1)) + n_patches_last
        assert(totalPatches == totalPatches_json)
        print(n_patches_first)

        if n_patches_first != n_patches_last:
            first_patchid = array1d(n_patches_first, psize)
            first_patchid = np.tile(np.array(first_patchid).flatten(), fileNames_250m.__len__()-1)

            last_patchid = array1d(n_patches_last, psize)
            last_patchid = np.array(last_patchid).flatten()

            final_patchid = np.concatenate((first_patchid,last_patchid))

        else:
            final_patchid = array1d(n_patches_first, psize)
            final_patchid = np.tile(np.array(final_patchid).flatten(), fileNames_250m.__len__()-1)

        split_patchid = np.split(final_patchid,(psize*psize)/2)
        final_patchid = [reshape_array(x, psize) for x in split_patchid]
        final_patchid = merge_tfrecord(final_patchid, patchesPerRow)

        write_tif(os.path.join(outdir,'patchid.tif'),final_patchid, crs, affine)

        first_fileid = np.repeat(list(range(fileNames_250m.__len__()-1)), n_patches_first * psize * psize)
        last_fileid = np.repeat(fileNames_250m.__len__()-1, n_patches_last * psize * psize)
        final_fileid = np.concatenate((first_fileid,last_fileid))

        split_fileid = np.split(final_fileid,(psize*psize)/2)
        final_fileid= [reshape_array(x, psize) for x in split_fileid]
        final_fileid = merge_tfrecord(final_fileid, patchesPerRow)

        write_tif(os.path.join(outdir,'fileid.tif'), final_fileid, crs, affine)

    elif fileNames_250m.__len__() == 1:

        n_patches = sum(1 for _ in tf.python_io.tf_record_iterator(fileNames_250m[0], options=options))
        assert(n_patches == totalPatches_json)

        final_patchid = array1d(n_patches, psize)
        final_patchid = np.array(final_patchid).flatten()

        split_patchid = np.split(final_patchid,(psize*psize)/2)
        final_patchid = [reshape_array(x, psize) for x in split_patchid]
        final_patchid = merge_tfrecord(final_patchid, patchesPerRow)

        write_tif(os.path.join(outdir,'patchid.tif'),final_patchid, crs, affine)

        final_fileid = np.repeat(fileNames_250m.__len__()-1, n_patches * psize * psize)
        split_fileid = np.split(final_fileid,(psize*psize)/2)
        final_fileid= [reshape_array(x, psize) for x in split_fileid]
        final_fileid = merge_tfrecord(final_fileid, patchesPerRow)

        write_tif(os.path.join(outdir,'fileid.tif'),final_fileid, crs, affine)

    else:
        print("No files found, check input raw directory")